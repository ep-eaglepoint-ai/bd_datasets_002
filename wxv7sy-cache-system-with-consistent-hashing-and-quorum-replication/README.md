# WXV7SY - Cache System with Consistent Hashing and Quorum Replication

**Category:** sft

## Overview
- Task ID: WXV7SY
- Title: Cache System with Consistent Hashing and Quorum Replication
- Category: sft
- Repository: ep-eaglepoint-ai/bd_datasets_002
- Branch: wxv7sy-cache-system-with-consistent-hashing-and-quorum-replication

## Requirements
- 1. Consistent Hashing Ring Implementation - The hash ring must support exactly 150 virtual nodes per physical node to ensure even key distribution across the cluster. - The hash function must use SHA-256 and map keys to a 2^32 address space (0 to 4,294,967,295). - When building the hash ring for 10 physical nodes (1,500 total virtual nodes), construction must complete in < 500ms. - Key lookup must be O(log N) using binary search on the sorted ring of (hash_value, node_id) tuples. - When adding an 11th node to a cluster with 1 million keys, only ~90,909 keys (1M/11) must migrate, not all keys. - The ring must be stored as a sorted list of tuples: [(hash_value_1, node_id_1), (hash_value_2, node_id_2), ...]. - When a key hashes to value H, the responsible node is the first node in the ring where hash_value >= H (clockwise successor). - Key distribution variance across nodes must be Â±5% or less (e.g., 10 nodes with 1M keys: each node holds 90K-110K keys). - Ring construction must be deterministic: same nodes always produce the same ring structure.  ### 2. Quorum-Based Data Replication - Each key must be replicated to exactly 3 nodes: the primary node (clockwise successor of key hash) plus the next 2 clockwise successors. - Quorum write (W=2): A SET operation succeeds when at least 2 out of 3 replicas acknowledge the write within 100ms. - Quorum read (R=2): A GET operation succeeds when at least 2 out of 3 replicas return the same version within 50ms. - Strong consistency guarantee: R + W > N (2 + 2 > 3) ensures reads always see the latest committed write. - Each cache entry must include: {key: str, value: bytes, timestamp: float, version: int}. - When a SET writes to replicas with versions [5, 5, 4], the write increments to version 6 and succeeds when 2 replicas ACK. - When a GET reads replicas with versions [5, 5, 4], it returns version 5 data and triggers async read-repair to update the stale replica. - Replication latency must be p99 < 100ms (time from coordinator sending write to receiving 2 ACKs). - When only 1 out of 3 replicas is reachable, both GET and SET must fail with error: "Quorum unavailable: only 1/3 replicas reachable".
- Cache Operations (GET, SET, DELETE) - SET operation: (1) hash key to find 3 replicas, (2) send write to all 3 in parallel, (3) wait for 2 ACKs, (4) return success if quorum met within 100ms. - GET operation: (1) hash key to find 3 replicas, (2) send read to all 3 in parallel, (3) wait for 2 responses, (4) return majority version if quorum met within 50ms. - DELETE operation: (1) hash key to find 3 replicas, (2) send delete to all 3 in parallel, (3) wait for 2 ACKs, (4) return success if quorum met within 100ms. - When GET receives responses with versions [7, 7, 5], it must: (1) return version 7 data immediately, (2) trigger async read-repair to update replica with version 5. - Read-repair must complete within 500ms: send latest version to stale replica, wait for ACK, log completion. - When a SET operation times out waiting for quorum (> 100ms), it must return error: "Write timeout: only 1/3 replicas responded". - When a GET operation receives conflicting versions [5, 6] from 2 replicas, it must return the higher version (6) and repair the stale replica (5). - SET latency must be p99 < 10ms under normal load (< 10,000 ops/sec per node). - GET latency must be p99 < 5ms under normal load (< 10,000 ops/sec per node).  ### 4. Node Addition and Key Rebalancing - When a new node joins the cluster, only keys that hash to the new node's virtual node positions must migrate, not all keys. - For a 10-node cluster with 1 million keys, adding an 11th node must migrate approximately 90,909 keys (1M / 11). - Key migration must happen in the background without blocking GET/SET/DELETE operations on non-migrating keys. - During migration, GET operations must check both the old owner and new owner: return data from whichever responds first. - Migration rate must be at least 3,333 keys/second to complete 90,909 keys within 30 seconds. - Full rebalancing (migrating all affected keys) must complete within 5 minutes for 1 million keys. - Migration process: (1) identify keys to migrate, (2) copy key to new owner, (3) verify copy succeeded, (4) delete from old owner, (5) update routing. - When a key is being migrated and a SET arrives, the SET must be applied to both old and new owners to prevent data loss. - After migration completes, the hash ring must be updated atomically to route all future requests to the new owner.
- Failure Detection via Heartbeat Monitoring - Each node must send a heartbeat message to all other nodes every 1 second containing: {node_id, timestamp, status}. - When a node misses 3 consecutive heartbeats (3 seconds), it must be marked as "suspected" but not yet failed. - When a node misses 5 consecutive heartbeats (5 seconds), it must be marked as "failed" and removed from the hash ring. - When a node is marked as failed, the next clockwise successor must be automatically promoted as a replica for all keys that were replicated to the failed node. - Replica promotion must complete within 10 seconds: (1) identify affected keys, (2) find new replica, (3) copy data, (4) update replication metadata. - When a failed node recovers and rejoins, it must sync its data from current replicas before being added back to the hash ring. - Recovery sync must transfer keys at a rate of at least 833 keys/second to sync 100,000 keys within 120 seconds. - Heartbeat messages must be sent over UDP to minimize overhead (< 100 bytes per heartbeat). - When a network partition occurs and a node cannot reach 50%+ of the cluster, it must mark itself as isolated and reject writes (fail-safe mode).  ### 6. Memory Management and LRU Eviction - Each node must enforce a configurable memory limit (e.g., 1024 MB) and track current memory usage. - Memory overhead per key must be < 100 bytes (excluding value size): key (32B) + timestamp (8B) + version (8B) + metadata (52B). - When memory usage exceeds 95% of the limit, the node must evict 10% of the least recently used (LRU) keys. - LRU tracking must use a doubly-linked list + hash map: O(1) access, O(1) update on GET/SET. - When a key is accessed (GET or SET), it must be moved to the head of the LRU list. - When evicting keys, the node must: (1) remove from LRU tail, (2) delete from hash map, (3) send DELETE to other replicas, (4) update memory usage. - Eviction must complete within 100ms to avoid blocking incoming requests. - When a node evicts a key, it must log: {timestamp, key, reason: "memory_limit", memory_before, memory_after}.
- Concurrent Access and Thread Safety - All cache operations (GET, SET, DELETE) must be thread-safe and support 1,000+ concurrent requests without data corruption. - The hash ring must be protected by a read-write lock: RLock for lookups, Lock for updates (node addition/removal). - Each key's cache entry must be protected by a per-key lock to prevent lost updates during concurrent SETs. - When 100 threads SET the same key simultaneously, all 100 must succeed and the final version must be 100 (no lost updates). - When 1,000 threads GET the same key simultaneously, all must complete within 10ms total (< 0.01ms per thread). - Lock contention must be < 5% of total execution time under normal load (measured via profiling). - The system must pass Python's threading race condition tests with zero data corruption.  ### 8. Network Communication and Retry Logic - All inter-node communication (replication, heartbeat, migration) must use TCP sockets with connection pooling (max 100 connections per node). - When a replica is unreachable, the coordinator must retry with exponential backoff: 10ms, 20ms, 40ms (max 3 retries). - After 3 failed retries (total 70ms), the replica must be marked as unreachable and excluded from quorum calculation. - Network messages must use a compact binary format (JSON or MessagePack) to minimize bandwidth: < 1KB per message. - When sending a write to 3 replicas, the coordinator must send all 3 requests in parallel (not sequential) to minimize latency. - Socket timeouts must be configured: connect timeout 100ms, read timeout 50ms for GET, 100ms for SET. - When a socket connection fails, it must be removed from the connection pool and a new connection established on next request.
- Clock Skew Detection and Handling - Each node must include its local timestamp in all messages (heartbeat, replication, migration). - When a node receives a message with timestamp > local_time + 5 seconds, it must log an error: "Clock skew detected: remote={remote_time}, local={local_time}". - Version numbers must be used for conflict resolution instead of timestamps to avoid clock skew issues. - When two replicas have conflicting versions due to concurrent writes, the higher version number wins (last-write-wins). - Timestamps must be stored with millisecond precision (float with 3 decimal places).  ### 10. Configuration and Initialization - The system must load configuration from a JSON file with the following structure:   ```json   {     "nodes": [       {"id": "node1", "host": "10.0.1.1", "port": 8001},       {"id": "node2", "host": "10.0.1.2", "port": 8001}     ],     "replication_factor": 3,     "virtual_nodes": 150,     "memory_limit_mb": 1024,     "heartbeat_interval_sec": 1,     "failure_threshold": 5   }   ``` - On startup, each node must: (1) load config, (2) build hash ring, (3) start heartbeat thread, (4) start request handler thread. - Configuration changes (adding/removing nodes) must be applied without restarting the cluster (hot reload). - When a configuration error is detected (e.g., replication_factor > num_nodes), the system must log error and use default values.
- 11. Error Handling and Logging - All errors must be logged with severity level (INFO, WARNING, ERROR, CRITICAL) and include: timestamp, node_id, operation, error_message. - When quorum is not met, return error: "Quorum unavailable: only X/3 replicas reachable" where X is the actual count. - When a hash collision is detected (two keys hash to same value), log warning: "Hash collision detected: key1={key1}, key2={key2}, hash={hash}". - When memory limit is exceeded, log warning: "Memory limit exceeded: current={current_mb}MB, limit={limit_mb}MB, evicting 10% of keys". - When a node fails, log error: "Node failed: node_id={node_id}, last_heartbeat={timestamp}, promoting replicas". - All exceptions must be caught and logged, never crash the process.  ### 12. Performance and Scalability Requirements - GET latency: p50 < 2ms, p99 < 5ms, p999 < 10ms under normal load (< 10,000 ops/sec per node). - SET latency: p50 < 5ms, p99 < 10ms, p999 < 20ms under normal load (< 10,000 ops/sec per node). - Throughput: Each node must handle at least 10,000 ops/sec (GET + SET + DELETE combined). - Cluster throughput must scale linearly: 10 nodes = 100,000 ops/sec, 20 nodes = 200,000 ops/sec. - Hash ring lookup must be < 1ms p99 using binary search on sorted list. - Memory overhead per key must be < 100 bytes (excluding value size). - Key migration rate must be at least 3,333 keys/second during rebalancing. - CPU utilization must be < 70% per node at 10,000 ops/sec sustained load.

## Metadata
- Programming Languages: Python
- Frameworks: (none)
- Libraries: (none)
- Databases: (none)
- Tools: (none)
- Best Practices: (none)
- Performance Metrics: (none)
- Security Standards: (none)

## Structure
- repository_before/: baseline code (`__init__.py`)
- repository_after/: optimized code (`__init__.py`)
- tests/: test suite (`__init__.py`)
- evaluation/: evaluation scripts (`evaluation.py`)
- instances/: sample/problem instances (JSON)
- patches/: patches for diffing
- trajectory/: notes or write-up (Markdown)

## Quick start
- Run tests locally: `python -m pytest -q tests`
- With Docker: `docker compose up --build --abort-on-container-exit`
- Add dependencies to `requirements.txt`

## Notes
- Keep commits focused and small.
- Open a PR when ready for review.
