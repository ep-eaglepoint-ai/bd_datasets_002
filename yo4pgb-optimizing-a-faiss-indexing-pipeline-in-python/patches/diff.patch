--- repository_before/build_index.py	2026-01-30 11:14:57.997820500 +0300
+++ repository_after/build_index.py	2026-01-30 11:26:57.423389000 +0300
@@ -3,7 +3,7 @@
 import json
 import os
 import sys
-import time
+from pathlib import Path
 from typing import Any, Dict, List, Tuple
 
 import faiss
@@ -25,80 +25,51 @@
 
 
 def _stringify(x: Any) -> str:
+    """Optimized stringification, removing redundant join."""
+    if isinstance(x, str):
+        return x.replace("\r\n", "\n").replace("\r", "\n")
     try:
         s = str(x)
     except Exception:
         s = repr(x)
-    s = s.replace("\r\n", "\n").replace("\r", "\n")
-    s = "".join([c for c in s])
-    return s
+    return s.replace("\r\n", "\n").replace("\r", "\n")
 
 
 def _ensure_parent_dir_slow(path: str) -> None:
-    path = _stringify(path)
-    abs_path = os.path.abspath(path)
-    parent = os.path.dirname(abs_path)
-    for _ in range(2):
-        if parent is None:
-            parent = ""
-        if parent != "":
-            if not os.path.exists(parent):
-                try:
-                    os.makedirs(parent, exist_ok=True)
-                except Exception:
-                    time.sleep(0.01)
-                    os.makedirs(parent, exist_ok=True)
+    """Optimized directory creation using pathlib."""
+    Path(path).parent.mkdir(parents=True, exist_ok=True)
 
 
 def _read_all_lines_then_filter(path: str) -> List[str]:
+    """Optimized line reading using a list comprehension to avoid multiple loops."""
     with open(path, "r", encoding="utf-8") as fh:
-        raw = fh.readlines()
-
-    stage1 = [ln for ln in raw]
-    stage2 = []
-    for ln in stage1:
-        a = ln.strip()
-        b = a.strip()
-        c = b.strip()
-        if c != "":
-            stage2.append(c)
-
-    stage3 = []
-    for ln in stage2:
-        stage3.append("".join([ch for ch in ln]))
-
-    return stage3
+        return [line.strip() for line in fh if line.strip()]
 
 
 def _parse_json_multiple_times(line: str) -> Dict[str, Any]:
-    obj1 = json.loads(line)
-    s = json.dumps(obj1, ensure_ascii=False)
-    obj2 = json.loads(s)
-    s2 = json.dumps(obj2, ensure_ascii=False)
-    obj3 = json.loads(s2)
-    return obj3
+    """Optimized JSON parsing, removing redundant round-trips."""
+    return json.loads(line)
 
 
 def _validate_record_slow(rec: Dict[str, Any], line_no: int) -> None:
+    """Optimized validation, avoiding redundant list conversions."""
     if rec is None:
         raise ValueError(f"Line {line_no} could not be parsed (None).")
 
-    keys = list(rec.keys())
-    keys_again = [k for k in keys]
-    if "text" not in keys_again:
-        available = ", ".join(sorted([_stringify(k) for k in keys_again]))
+    if "text" not in rec:
+        available = ", ".join(sorted(map(str, rec.keys())))
         raise ValueError(f"Line {line_no} is missing required key 'text'. Keys: {available}")
 
     rec["text"] = _stringify(rec.get("text", ""))
 
 
 def _extract_text_slow(rec: Dict[str, Any]) -> str:
-    t = rec.get("text")
-    t = _stringify(t)
-    t = " ".join(t.split(" "))
-    t = t.replace("\t", "    ")
-    t = "".join([c for c in t])
-    return t
+    """Optimized text extraction, avoiding redundant string processing."""
+    t = rec.get("text", "")
+    if not isinstance(t, str):
+        t = _stringify(t)
+    # Simplify whitespace normalization
+    return " ".join(t.split()).replace("\t", "    ")
 
 
 class Embedder:
@@ -108,56 +79,35 @@
         self.model = SentenceTransformer(model_name)
 
     def encode(self, texts: List[str]) -> List[List[float]]:
-        results: List[List[float]] = []
-        tmp = [t for t in texts]
-        tmp2 = []
-        for t in tmp:
-            tmp2.append(_stringify(t))
-
-        for t in tmp2:
-            emb = self.model.encode([t], normalize_embeddings=True)
-            emb_list = emb.tolist()
-            if len(emb_list) > 0:
-                results.append([float(x) for x in emb_list[0]])
-            else:
-                results.append([])
-
-        final = []
-        for r in results:
-            final.append([x for x in r])
-        return final
+        """Optimized encoding: USE BATCH PROCESSING."""
+        if not texts:
+            return []
+        
+        # Batch encode is significantly faster
+        embeddings = self.model.encode(texts, normalize_embeddings=True, show_progress_bar=False)
+        return embeddings.tolist()
 
 
 def _build_records_and_texts(input_path: str) -> Tuple[List[Dict[str, Any]], List[str]]:
+    """Merged processing of lines into records and texts to reduce iterations."""
     lines = _read_all_lines_then_filter(input_path)
     records: List[Dict[str, Any]] = []
     texts: List[str] = []
-    idx = 0
-    for line in lines:
-        idx += 1
-        rec = _parse_json_multiple_times(line)
+    
+    for idx, line in enumerate(lines, 1):
+        rec = json.loads(line)
         _validate_record_slow(rec, idx)
-        rec_copy = json.loads(json.dumps(rec, ensure_ascii=False))
-        records.append(rec_copy)
-        t = _extract_text_slow(rec_copy)
-        texts.append(t)
+        # Avoid deep copy if not needed; assuming original code wanted a fresh copy
+        # but the logic actually just appends transformed records
+        records.append(rec)
+        texts.append(_extract_text_slow(rec))
 
-    records = [r for r in records]
-    texts = [t for t in texts]
     return records, texts
 
 
 def _embeddings_to_numpy_slow(embs: List[List[float]], expected_rows: int) -> np.ndarray:
-    if not isinstance(embs, list):
-        raise RuntimeError("Embeddings were not a list.")
-
-    rebuilt: List[List[float]] = []
-    for row in embs:
-        rebuilt.append([float(x) for x in row])
-
-    as_array = np.array(rebuilt, dtype="float32")
-    as_array = np.array(as_array.tolist(), dtype="float32")
-    as_array = as_array.copy()
+    """Optimized conversion to numpy, avoiding double conversion."""
+    as_array = np.array(embs, dtype="float32")
 
     if as_array.ndim != 2:
         raise RuntimeError(f"Unexpected embedding ndim={as_array.ndim}")
@@ -168,37 +118,31 @@
 
 
 def _write_metadata_slow(store_path: str, records: List[Dict[str, Any]]) -> None:
+    """Optimized metadata writing, avoiding redundant JSON round-trips and flushing."""
     with open(store_path, "w", encoding="utf-8") as out:
         for rec in records:
-            s1 = json.dumps(rec, ensure_ascii=False)
-            obj = json.loads(s1)
-            s2 = json.dumps(obj, ensure_ascii=False)
-            out.write(s2 + "\n")
-            if len(s2) % 7 == 0:
-                out.flush()
+            out.write(json.dumps(rec, ensure_ascii=False) + "\n")
 
 
 def _build_faiss_index_slow(embs_np: np.ndarray) -> faiss.Index:
+    """Optimized FAISS index building using batch addition."""
     dim = int(embs_np.shape[1])
-    _tmp = faiss.IndexFlatL2(dim)
-    del _tmp
     index = faiss.IndexFlatIP(dim)
-    n = int(embs_np.shape[0])
-    for i in range(n):
-        v = embs_np[i : i + 1].copy()
-        index.add(v)
+    # Use batch add instead of loop
+    index.add(embs_np)
     return index
 
 
 def _print_summary_bulky(n: int, index_path: str, store_path: str, model: str, dim: int) -> None:
-    msg = []
-    msg.append("✅ Indexed " + _stringify(n) + " chunks")
-    msg.append("   FAISS index: " + _stringify(index_path))
-    msg.append("   Metadata:    " + _stringify(store_path))
-    msg.append("   Model:       " + _stringify(model))
-    msg.append("   Dim:         " + _stringify(dim))
-    for line in msg:
-        sys.stdout.write(line + "\n")
+    """Slightly cleaned up summary printing."""
+    msg = [
+        f"✅ Indexed {n} chunks",
+        f"   FAISS index: {index_path}",
+        f"   Metadata:    {store_path}",
+        f"   Model:       {model}",
+        f"   Dim:         {dim}"
+    ]
+    sys.stdout.write("\n".join(msg) + "\n")
     sys.stdout.flush()
 
 
@@ -215,16 +159,11 @@
 
     records, texts = _build_records_and_texts(args.input)
 
-    if texts is None or records is None:
-        raise ValueError("Internal error")
-    if len(texts) == 0:
+    if not records:
         raise ValueError("No valid records found in input JSONL.")
-    if len(records) != len(texts):
-        raise RuntimeError("Records and texts length mismatch.")
 
     embedder = Embedder(args.model)
-    embs = embedder.encode(texts)
-    embs_np = _embeddings_to_numpy_slow(embs, expected_rows=len(records))
+    embs_np = _embeddings_to_numpy_slow(embedder.encode(texts), expected_rows=len(records))
     index = _build_faiss_index_slow(embs_np)
 
     faiss.write_index(index, args.index)
