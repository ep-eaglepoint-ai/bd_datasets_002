diff --git a/repository_before/__init__.py b/repository_after/__init__.py
index e69de29b..50cc6985 100644
--- a/repository_before/__init__.py
+++ b/repository_after/__init__.py
@@ -0,0 +1,3 @@
+from .api import app
+
+__all__ = ["app"]
diff --git a/repository_after/__pycache__/__init__.cpython-311.pyc b/repository_after/__pycache__/__init__.cpython-311.pyc
new file mode 100644
index 00000000..1e78eeff
Binary files /dev/null and b/repository_after/__pycache__/__init__.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/api.cpython-311.pyc b/repository_after/__pycache__/api.cpython-311.pyc
new file mode 100644
index 00000000..a77eb472
Binary files /dev/null and b/repository_after/__pycache__/api.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/celery_app.cpython-311.pyc b/repository_after/__pycache__/celery_app.cpython-311.pyc
new file mode 100644
index 00000000..6a2960d2
Binary files /dev/null and b/repository_after/__pycache__/celery_app.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/config.cpython-311.pyc b/repository_after/__pycache__/config.cpython-311.pyc
new file mode 100644
index 00000000..07477885
Binary files /dev/null and b/repository_after/__pycache__/config.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/db.cpython-311.pyc b/repository_after/__pycache__/db.cpython-311.pyc
new file mode 100644
index 00000000..4ffbab87
Binary files /dev/null and b/repository_after/__pycache__/db.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/models.cpython-311.pyc b/repository_after/__pycache__/models.cpython-311.pyc
new file mode 100644
index 00000000..ca102504
Binary files /dev/null and b/repository_after/__pycache__/models.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/schemas.cpython-311.pyc b/repository_after/__pycache__/schemas.cpython-311.pyc
new file mode 100644
index 00000000..24dbadf2
Binary files /dev/null and b/repository_after/__pycache__/schemas.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/storage.cpython-311.pyc b/repository_after/__pycache__/storage.cpython-311.pyc
new file mode 100644
index 00000000..ee042c8d
Binary files /dev/null and b/repository_after/__pycache__/storage.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/tasks.cpython-311.pyc b/repository_after/__pycache__/tasks.cpython-311.pyc
new file mode 100644
index 00000000..a1eb7add
Binary files /dev/null and b/repository_after/__pycache__/tasks.cpython-311.pyc differ
diff --git a/repository_after/__pycache__/webhook.cpython-311.pyc b/repository_after/__pycache__/webhook.cpython-311.pyc
new file mode 100644
index 00000000..3e8d2c5b
Binary files /dev/null and b/repository_after/__pycache__/webhook.cpython-311.pyc differ
diff --git a/repository_after/api.py b/repository_after/api.py
new file mode 100644
index 00000000..90d571d9
--- /dev/null
+++ b/repository_after/api.py
@@ -0,0 +1,273 @@
+from __future__ import annotations
+
+import uuid
+import asyncio
+from contextlib import asynccontextmanager
+from datetime import datetime
+
+from fastapi import Depends, FastAPI, File, Form, HTTPException, Request, Response, UploadFile
+from fastapi.responses import JSONResponse
+from fastapi.exceptions import RequestValidationError
+from sqlalchemy import delete, func, select
+from sqlalchemy.ext.asyncio import AsyncSession
+
+from .config import settings
+from .db import create_engine, create_sessionmaker, get_session
+from .models import Base, Job, JobStatus, ProcessingError
+from .schemas import (
+    HealthResponse,
+    JobsListResponse,
+    JobResponse,
+    ProcessingErrorsListResponse,
+    UploadResponse,
+)
+from .storage import job_storage_path, sanitize_filename, stream_upload_to_disk
+from .tasks import check_redis_health, process_job
+
+
+def _file_type_from_name(filename: str) -> str:
+    parts = filename.rsplit(".", 1)
+    if len(parts) != 2:
+        return ""
+    return parts[1].lower()
+
+
+@asynccontextmanager
+async def lifespan(app: FastAPI):
+    engine = create_engine()
+    app.state.engine = engine
+    app.state.sessionmaker = create_sessionmaker(engine)
+
+    if settings.auto_create_db:
+        async with engine.begin() as conn:
+            await conn.run_sync(Base.metadata.create_all)
+
+    yield
+
+    await engine.dispose()
+
+
+app = FastAPI(title=settings.app_name, lifespan=lifespan)
+
+
+@app.exception_handler(HTTPException)
+async def http_exception_handler(_: Request, exc: HTTPException):
+    return JSONResponse(status_code=exc.status_code, content={"detail": exc.detail})
+
+
+@app.exception_handler(RequestValidationError)
+async def validation_exception_handler(_: Request, exc: RequestValidationError):
+    # Requirement: 400 for validation errors (instead of FastAPI default 422)
+    return JSONResponse(status_code=400, content={"detail": exc.errors()})
+
+
+@app.post("/api/files/upload", response_model=UploadResponse)
+async def upload_file(
+    request: Request,
+    file: UploadFile = File(...),
+    webhook_url: str | None = Form(default=None),
+    session: AsyncSession = Depends(get_session),
+):
+    filename = sanitize_filename(file.filename or "upload")
+    file_type = _file_type_from_name(filename)
+    if file_type not in {"csv", "xlsx", "xls"}:
+        raise HTTPException(status_code=400, detail="Unsupported file type")
+
+    job_id = uuid.uuid4()
+    dest = job_storage_path(str(job_id), filename)
+
+    # Stream to disk in 8KB chunks and enforce max size.
+    file_size = await stream_upload_to_disk(file, dest, settings.max_upload_bytes)
+
+    job = Job(
+        id=job_id,
+        filename=filename,
+        file_size=file_size,
+        file_type=file_type,
+        status=JobStatus.QUEUED,
+        webhook_url=webhook_url,
+        progress=0,
+        rows_processed=0,
+        rows_failed=0,
+    )
+    session.add(job)
+    await session.commit()
+
+    # Enqueue background processing (Celery)
+    if settings.celery_task_always_eager:
+        from .tasks import _process_job_async
+
+        asyncio.create_task(_process_job_async(str(job_id)))
+    else:
+        process_job.delay(str(job_id))
+
+    return UploadResponse(job_id=job_id)
+
+
+@app.get("/api/jobs", response_model=JobsListResponse)
+async def list_jobs(
+    status: JobStatus | None = None,
+    from_date: datetime | None = None,
+    to_date: datetime | None = None,
+    page: int = 1,
+    page_size: int = 50,
+    session: AsyncSession = Depends(get_session),
+):
+    if page < 1 or page_size < 1 or page_size > 200:
+        raise HTTPException(status_code=400, detail="Invalid pagination")
+
+    base = select(Job)
+    count_q = select(func.count()).select_from(Job)
+
+    if status is not None:
+        base = base.where(Job.status == status)
+        count_q = count_q.where(Job.status == status)
+    if from_date is not None:
+        base = base.where(Job.created_at >= from_date)
+        count_q = count_q.where(Job.created_at >= from_date)
+    if to_date is not None:
+        base = base.where(Job.created_at <= to_date)
+        count_q = count_q.where(Job.created_at <= to_date)
+
+    total = (await session.execute(count_q)).scalar_one()
+    total_pages = max(1, (total + page_size - 1) // page_size) if total else 1
+
+    q = (
+        base.order_by(Job.created_at.desc())
+        .offset((page - 1) * page_size)
+        .limit(page_size)
+    )
+
+    jobs = (await session.execute(q)).scalars().all()
+
+    return JobsListResponse(
+        total=total,
+        page=page,
+        page_size=page_size,
+        total_pages=total_pages,
+        jobs=[JobResponse.model_validate(j) for j in jobs],
+    )
+
+
+@app.get("/api/jobs/{job_id}", response_model=JobResponse)
+async def get_job(job_id: uuid.UUID, session: AsyncSession = Depends(get_session)):
+    job = (await session.execute(select(Job).where(Job.id == job_id))).scalar_one_or_none()
+    if not job:
+        raise HTTPException(status_code=404, detail="Job not found")
+    return JobResponse.model_validate(job)
+
+
+@app.get("/api/jobs/{job_id}/errors", response_model=ProcessingErrorsListResponse)
+async def get_job_errors(
+    job_id: uuid.UUID,
+    page: int = 1,
+    page_size: int = 50,
+    session: AsyncSession = Depends(get_session),
+):
+    if page < 1 or page_size < 1 or page_size > 500:
+        raise HTTPException(status_code=400, detail="Invalid pagination")
+
+    job = (await session.execute(select(Job.id).where(Job.id == job_id))).scalar_one_or_none()
+    if not job:
+        raise HTTPException(status_code=404, detail="Job not found")
+
+    count_q = select(func.count()).select_from(ProcessingError).where(ProcessingError.job_id == job_id)
+    total = (await session.execute(count_q)).scalar_one()
+    total_pages = max(1, (total + page_size - 1) // page_size) if total else 1
+
+    q = (
+        select(ProcessingError)
+        .where(ProcessingError.job_id == job_id)
+        .order_by(ProcessingError.row_number.asc())
+        .offset((page - 1) * page_size)
+        .limit(page_size)
+    )
+    errors = (await session.execute(q)).scalars().all()
+
+    from .schemas import ProcessingErrorResponse
+
+    return ProcessingErrorsListResponse(
+        total=total,
+        page=page,
+        page_size=page_size,
+        total_pages=total_pages,
+        errors=[ProcessingErrorResponse.model_validate(e) for e in errors],
+    )
+
+
+@app.delete("/api/jobs/{job_id}")
+async def delete_job(job_id: uuid.UUID, session: AsyncSession = Depends(get_session)):
+    job = (await session.execute(select(Job).where(Job.id == job_id))).scalar_one_or_none()
+    if not job:
+        raise HTTPException(status_code=404, detail="Job not found")
+
+    if job.status == JobStatus.PROCESSING:
+        job.status = JobStatus.CANCELLED
+        await session.commit()
+        return Response(status_code=204)
+
+    # Delete file and DB records
+    path = job_storage_path(str(job_id), job.filename)
+    try:
+        path.unlink(missing_ok=True)
+    except Exception:
+        pass
+
+    await session.execute(delete(Job).where(Job.id == job_id))
+    await session.commit()
+
+    return Response(status_code=204)
+
+
+@app.post("/api/jobs/{job_id}/retry", response_model=JobResponse)
+async def retry_job(job_id: uuid.UUID, session: AsyncSession = Depends(get_session)):
+    job = (await session.execute(select(Job).where(Job.id == job_id))).scalar_one_or_none()
+    if not job:
+        raise HTTPException(status_code=404, detail="Job not found")
+
+    if job.status not in {JobStatus.FAILED, JobStatus.CANCELLED}:
+        raise HTTPException(status_code=400, detail="Job is not retryable")
+
+    # Clear errors
+    await session.execute(delete(ProcessingError).where(ProcessingError.job_id == job_id))
+
+    job.status = JobStatus.QUEUED
+    job.progress = 0
+    job.rows_processed = 0
+    job.rows_failed = 0
+    job.error_message = None
+    job.started_at = None
+    job.completed_at = None
+
+    await session.commit()
+
+    if settings.celery_task_always_eager:
+        from .tasks import _process_job_async
+
+        asyncio.create_task(_process_job_async(str(job_id)))
+    else:
+        process_job.delay(str(job_id))
+
+    return JobResponse.model_validate(job)
+
+
+@app.get("/api/health", response_model=HealthResponse)
+async def health(request: Request, session: AsyncSession = Depends(get_session)):
+    db_ok = False
+    redis_ok = False
+    details: dict[str, object] = {}
+
+    try:
+        await session.execute(select(1))
+        db_ok = True
+    except Exception as exc:  # noqa: BLE001
+        details["db_error"] = str(exc)
+
+    redis_ok = await check_redis_health(settings.redis_url)
+    if not redis_ok:
+        details["redis_error"] = "Redis ping failed"
+
+    status = "healthy" if (db_ok and redis_ok) else "unhealthy"
+    if status == "healthy":
+        return HealthResponse(status=status, db=db_ok, redis=redis_ok, details=details)
+    return JSONResponse(status_code=503, content=HealthResponse(status=status, db=db_ok, redis=redis_ok, details=details).model_dump())
diff --git a/repository_after/celery_app.py b/repository_after/celery_app.py
new file mode 100644
index 00000000..db371ba8
--- /dev/null
+++ b/repository_after/celery_app.py
@@ -0,0 +1,23 @@
+from __future__ import annotations
+
+from celery import Celery
+
+from .config import settings
+
+
+celery_app = Celery(
+    "file_processor",
+    broker=settings.resolved_broker_url(),
+    backend=settings.resolved_backend_url(),
+)
+
+celery_app.conf.update(
+    task_acks_late=True,
+    worker_prefetch_multiplier=1,
+    worker_concurrency=4,
+    worker_max_tasks_per_child=10,
+    task_always_eager=settings.celery_task_always_eager,
+    task_soft_time_limit=settings.task_soft_time_limit_seconds,
+    task_time_limit=settings.task_time_limit_seconds,
+    worker_shutdown_timeout=settings.worker_shutdown_timeout_seconds,
+)
diff --git a/repository_after/config.py b/repository_after/config.py
new file mode 100644
index 00000000..e79c6fbe
--- /dev/null
+++ b/repository_after/config.py
@@ -0,0 +1,48 @@
+from __future__ import annotations
+
+from pathlib import Path
+from typing import Literal
+
+from pydantic import Field
+from pydantic_settings import BaseSettings, SettingsConfigDict
+
+
+class Settings(BaseSettings):
+    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8", extra="ignore")
+
+    app_name: str = "file-processing-microservice"
+
+    database_url: str = Field(
+        default="postgresql+asyncpg://postgres:postgres@db:5432/postgres",
+        alias="DATABASE_URL",
+    )
+
+    redis_url: str = Field(default="redis://redis:6379/0", alias="REDIS_URL")
+
+    upload_dir: Path = Field(default=Path("/data/uploads"), alias="UPLOAD_DIR")
+
+    max_upload_bytes: int = Field(default=500 * 1024 * 1024, alias="MAX_UPLOAD_BYTES")
+
+    auto_create_db: bool = Field(default=True, alias="AUTO_CREATE_DB")
+
+    # Celery
+    celery_broker_url: str | None = Field(default=None, alias="CELERY_BROKER_URL")
+    celery_result_backend: str | None = Field(default=None, alias="CELERY_RESULT_BACKEND")
+    celery_task_always_eager: bool = Field(default=False, alias="CELERY_TASK_ALWAYS_EAGER")
+
+    # Worker/task timing
+    progress_update_interval_seconds: float = Field(default=5.0, alias="PROGRESS_UPDATE_INTERVAL_SECONDS")
+    task_soft_time_limit_seconds: int = Field(default=3600, alias="TASK_SOFT_TIME_LIMIT_SECONDS")
+    task_time_limit_seconds: int = Field(default=3700, alias="TASK_TIME_LIMIT_SECONDS")
+    worker_shutdown_timeout_seconds: int = Field(default=60, alias="WORKER_SHUTDOWN_TIMEOUT_SECONDS")
+
+    def resolved_broker_url(self) -> str:
+        return self.celery_broker_url or self.redis_url
+
+    def resolved_backend_url(self) -> str:
+        return self.celery_result_backend or self.redis_url
+
+
+settings = Settings()
+
+FileType = Literal["csv", "xlsx", "xls"]
diff --git a/repository_after/db.py b/repository_after/db.py
new file mode 100644
index 00000000..c5de4220
--- /dev/null
+++ b/repository_after/db.py
@@ -0,0 +1,42 @@
+from __future__ import annotations
+
+from collections.abc import AsyncIterator
+
+from sqlalchemy.ext.asyncio import (
+    AsyncEngine,
+    AsyncSession,
+    async_sessionmaker,
+    create_async_engine,
+)
+
+from fastapi import Request
+
+from .config import settings
+
+
+def create_engine(database_url: str | None = None) -> AsyncEngine:
+    url = database_url or settings.database_url
+    if url.startswith("sqlite"):
+        return create_async_engine(
+            url,
+            pool_pre_ping=True,
+            future=True,
+        )
+
+    return create_async_engine(
+        url,
+        pool_size=20,
+        max_overflow=40,
+        pool_pre_ping=True,
+        future=True,
+    )
+
+
+def create_sessionmaker(engine: AsyncEngine) -> async_sessionmaker[AsyncSession]:
+    return async_sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)
+
+
+async def get_session(request: Request) -> AsyncIterator[AsyncSession]:
+    session_factory: async_sessionmaker[AsyncSession] = request.app.state.sessionmaker
+    async with session_factory() as session:
+        yield session
diff --git a/repository_after/models.py b/repository_after/models.py
new file mode 100644
index 00000000..77351f09
--- /dev/null
+++ b/repository_after/models.py
@@ -0,0 +1,74 @@
+from __future__ import annotations
+
+import enum
+import uuid
+
+from sqlalchemy import BigInteger, DateTime, ForeignKey, Index, Integer, String, Text, func
+from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship
+from sqlalchemy.types import Enum as SAEnum
+from sqlalchemy import Uuid
+
+
+class Base(DeclarativeBase):
+    pass
+
+
+class JobStatus(str, enum.Enum):
+    QUEUED = "QUEUED"
+    PROCESSING = "PROCESSING"
+    COMPLETED = "COMPLETED"
+    FAILED = "FAILED"
+    CANCELLED = "CANCELLED"
+
+
+class Job(Base):
+    __tablename__ = "jobs"
+
+    id: Mapped[uuid.UUID] = mapped_column(Uuid(as_uuid=True), primary_key=True, default=uuid.uuid4)
+    filename: Mapped[str] = mapped_column(String(255), nullable=False)
+    file_size: Mapped[int] = mapped_column(BigInteger, nullable=False)
+    file_type: Mapped[str] = mapped_column(String(50), nullable=False)
+    status: Mapped[JobStatus] = mapped_column(SAEnum(JobStatus, name="job_status"), nullable=False, default=JobStatus.QUEUED)
+
+    progress: Mapped[int] = mapped_column(Integer, nullable=False, default=0)
+    rows_processed: Mapped[int] = mapped_column(Integer, nullable=False, default=0)
+    rows_failed: Mapped[int] = mapped_column(Integer, nullable=False, default=0)
+
+    error_message: Mapped[str | None] = mapped_column(Text, nullable=True)
+    webhook_url: Mapped[str | None] = mapped_column(String(500), nullable=True)
+
+    created_at: Mapped[object] = mapped_column(DateTime(timezone=True), server_default=func.now(), nullable=False)
+    started_at: Mapped[object | None] = mapped_column(DateTime(timezone=True), nullable=True)
+    completed_at: Mapped[object | None] = mapped_column(DateTime(timezone=True), nullable=True)
+
+    errors: Mapped[list[ProcessingError]] = relationship(
+        "ProcessingError",
+        back_populates="job",
+        cascade="all, delete-orphan",
+        passive_deletes=True,
+    )
+
+
+Index("ix_jobs_created_at", Job.created_at)
+
+
+class ProcessingError(Base):
+    __tablename__ = "processing_errors"
+
+    id: Mapped[uuid.UUID] = mapped_column(Uuid(as_uuid=True), primary_key=True, default=uuid.uuid4)
+    job_id: Mapped[uuid.UUID] = mapped_column(
+        Uuid(as_uuid=True),
+        ForeignKey("jobs.id", ondelete="CASCADE"),
+        nullable=False,
+        index=True,
+    )
+
+    row_number: Mapped[int] = mapped_column(Integer, nullable=False)
+    column_name: Mapped[str | None] = mapped_column(String(255), nullable=True)
+    error_type: Mapped[str] = mapped_column(String(100), nullable=False)
+    error_message: Mapped[str] = mapped_column(Text, nullable=False)
+    raw_value: Mapped[str | None] = mapped_column(Text, nullable=True)
+
+    created_at: Mapped[object] = mapped_column(DateTime(timezone=True), server_default=func.now(), nullable=False)
+
+    job: Mapped[Job] = relationship("Job", back_populates="errors")
diff --git a/repository_after/schemas.py b/repository_after/schemas.py
new file mode 100644
index 00000000..ed38e51f
--- /dev/null
+++ b/repository_after/schemas.py
@@ -0,0 +1,99 @@
+from __future__ import annotations
+
+import uuid
+from datetime import datetime
+from typing import Any, Literal
+
+from pydantic import BaseModel, Field, HttpUrl
+
+from .models import JobStatus
+
+
+class JobCreate(BaseModel):
+    filename: str = Field(..., max_length=255)
+    file_size: int = Field(..., ge=0)
+    file_type: Literal["csv", "xlsx", "xls"]
+    webhook_url: HttpUrl | None = Field(default=None)
+
+
+class JobUpdate(BaseModel):
+    status: JobStatus | None = None
+    progress: int | None = Field(default=None, ge=0, le=100)
+    rows_processed: int | None = Field(default=None, ge=0)
+    rows_failed: int | None = Field(default=None, ge=0)
+    error_message: str | None = None
+    webhook_url: HttpUrl | None = None
+
+
+class JobResponse(BaseModel):
+    model_config = {"from_attributes": True}
+
+    id: uuid.UUID
+    filename: str
+    file_size: int
+    file_type: str
+    status: JobStatus
+    progress: int
+    rows_processed: int
+    rows_failed: int
+    error_message: str | None
+    webhook_url: str | None
+    created_at: datetime
+    started_at: datetime | None
+    completed_at: datetime | None
+
+
+class JobsListResponse(BaseModel):
+    total: int
+    page: int
+    page_size: int
+    total_pages: int
+    jobs: list[JobResponse]
+
+
+class ProcessingErrorResponse(BaseModel):
+    model_config = {"from_attributes": True}
+
+    id: uuid.UUID
+    job_id: uuid.UUID
+    row_number: int
+    column_name: str | None
+    error_type: str
+    error_message: str
+    raw_value: str | None
+    created_at: datetime
+
+
+class ProcessingErrorCreate(BaseModel):
+    job_id: uuid.UUID
+    row_number: int = Field(..., ge=1)
+    column_name: str | None = Field(default=None, max_length=255)
+    error_type: str = Field(..., max_length=100)
+    error_message: str
+    raw_value: str | None = None
+
+
+class ProcessingErrorUpdate(BaseModel):
+    column_name: str | None = Field(default=None, max_length=255)
+    error_type: str | None = Field(default=None, max_length=100)
+    error_message: str | None = None
+    raw_value: str | None = None
+
+
+class ProcessingErrorsListResponse(BaseModel):
+    total: int
+    page: int
+    page_size: int
+    total_pages: int
+    errors: list[ProcessingErrorResponse]
+
+
+class UploadResponse(BaseModel):
+    job_id: uuid.UUID
+
+
+class HealthResponse(BaseModel):
+    status: Literal["healthy", "unhealthy"]
+    db: bool
+    redis: bool
+    details: dict[str, Any] = Field(default_factory=dict)
diff --git a/repository_after/storage.py b/repository_after/storage.py
new file mode 100644
index 00000000..e41344b8
--- /dev/null
+++ b/repository_after/storage.py
@@ -0,0 +1,56 @@
+from __future__ import annotations
+
+import re
+from pathlib import Path
+
+import aiofiles
+from fastapi import HTTPException, UploadFile
+
+from .config import settings
+
+
+_CHUNK_SIZE = 8 * 1024
+
+
+def sanitize_filename(name: str) -> str:
+    name = name.strip().replace("\\", "_").replace("/", "_")
+    name = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
+    name = name.strip("._")
+    if not name:
+        name = "upload"
+    return name[:255]
+
+
+def job_storage_path(job_id: str, filename: str) -> Path:
+    safe = sanitize_filename(filename)
+    return settings.upload_dir / f"{job_id}_{safe}"
+
+
+async def stream_upload_to_disk(file: UploadFile, dest: Path, max_bytes: int) -> int:
+    dest.parent.mkdir(parents=True, exist_ok=True)
+
+    total = 0
+    try:
+        async with aiofiles.open(dest, "wb") as out:
+            while True:
+                chunk = await file.read(_CHUNK_SIZE)
+                if not chunk:
+                    break
+                total += len(chunk)
+                if total > max_bytes:
+                    raise HTTPException(status_code=413, detail="File too large")
+                await out.write(chunk)
+    except HTTPException:
+        try:
+            dest.unlink(missing_ok=True)
+        except Exception:
+            pass
+        raise
+    except Exception as e:
+        try:
+            dest.unlink(missing_ok=True)
+        except Exception:
+            pass
+        raise HTTPException(status_code=500, detail=f"Failed to save upload: {e}")
+
+    return total
diff --git a/repository_after/tasks.py b/repository_after/tasks.py
new file mode 100644
index 00000000..ab0e9bfb
--- /dev/null
+++ b/repository_after/tasks.py
@@ -0,0 +1,330 @@
+from __future__ import annotations
+
+import asyncio
+import csv
+import time
+import uuid
+from datetime import datetime, timezone
+from pathlib import Path
+from typing import Any
+
+import pandas as pd
+from celery.exceptions import SoftTimeLimitExceeded
+from openpyxl import load_workbook
+from redis.asyncio import Redis
+from sqlalchemy import delete, select, update
+from sqlalchemy.ext.asyncio import AsyncSession
+
+from .celery_app import celery_app
+from .config import settings
+from .db import create_engine, create_sessionmaker
+from .models import Job, JobStatus, ProcessingError
+from .storage import job_storage_path
+from .webhook import deliver_webhook
+
+
+def _now() -> datetime:
+    return datetime.now(timezone.utc)
+
+
+def _is_null(value: Any) -> bool:
+    if value is None:
+        return True
+    try:
+        if pd.isna(value):
+            return True
+    except Exception:
+        pass
+    if isinstance(value, str) and value.strip() == "":
+        return True
+    return False
+
+
+def _validate_row(row: dict[str, Any]) -> list[tuple[str | None, str, str, str | None]]:
+    errors: list[tuple[str | None, str, str, str | None]] = []
+    for col, val in row.items():
+        if _is_null(val):
+            errors.append((col, "NULL", "Value is required", None if val is None else str(val)))
+        else:
+            if isinstance(val, str) and len(val) > 2000:
+                errors.append((col, "CONSTRAINT", "Value too long", val[:2000]))
+    return errors
+
+
+def _should_persist(last_persist: float, now: float, interval: float) -> bool:
+    return (now - last_persist) >= interval
+
+
+async def _get_job(session: AsyncSession, job_id: uuid.UUID) -> Job | None:
+    res = await session.execute(select(Job).where(Job.id == job_id))
+    return res.scalar_one_or_none()
+
+
+async def _update_job_progress(
+    session: AsyncSession,
+    job_id: uuid.UUID,
+    *,
+    status: JobStatus | None = None,
+    progress: int | None = None,
+    rows_processed: int | None = None,
+    rows_failed: int | None = None,
+    error_message: str | None = None,
+    started_at: datetime | None = None,
+    completed_at: datetime | None = None,
+) -> None:
+    values: dict[str, Any] = {}
+    if status is not None:
+        values["status"] = status
+    if progress is not None:
+        values["progress"] = max(0, min(100, int(progress)))
+    if rows_processed is not None:
+        values["rows_processed"] = int(rows_processed)
+    if rows_failed is not None:
+        values["rows_failed"] = int(rows_failed)
+    if error_message is not None:
+        values["error_message"] = error_message
+    if started_at is not None:
+        values["started_at"] = started_at
+    if completed_at is not None:
+        values["completed_at"] = completed_at
+
+    if not values:
+        return
+
+    await session.execute(update(Job).where(Job.id == job_id).values(**values))
+    await session.commit()
+
+
+async def _log_errors(
+    session: AsyncSession,
+    job_id: uuid.UUID,
+    row_number: int,
+    errors: list[tuple[str | None, str, str, str | None]],
+) -> None:
+    for (column_name, error_type, error_message, raw_value) in errors:
+        session.add(
+            ProcessingError(
+                job_id=job_id,
+                row_number=row_number,
+                column_name=column_name,
+                error_type=error_type,
+                error_message=error_message,
+                raw_value=raw_value,
+            )
+        )
+    await session.commit()
+
+
+async def _process_csv(path: Path, job_id: uuid.UUID, session: AsyncSession) -> tuple[int, int]:
+    # Determine total rows (excluding header) for progress.
+    total_rows = 0
+    with path.open("r", newline="", encoding="utf-8", errors="ignore") as f:
+        reader = csv.reader(f)
+        try:
+            next(reader)
+        except StopIteration:
+            total_rows = 0
+        else:
+            total_rows = sum(1 for _ in reader)
+
+    rows_seen = 0
+    rows_failed = 0
+
+    last_persist = time.monotonic()
+
+    for chunk in pd.read_csv(path, chunksize=10_000):
+        # cancellation check before chunk
+        job = await _get_job(session, job_id)
+        if not job or job.status == JobStatus.CANCELLED:
+            break
+
+        for _, row in chunk.iterrows():
+            rows_seen += 1
+            as_dict = {str(k): row[k] for k in row.index}
+            errs = _validate_row(as_dict)
+            if errs:
+                rows_failed += 1
+                await _log_errors(session, job_id, rows_seen, errs)
+
+        now = time.monotonic()
+        if _should_persist(last_persist, now, settings.progress_update_interval_seconds):
+            last_persist = now
+            progress = 100 if total_rows == 0 else int((rows_seen / max(total_rows, 1)) * 100)
+            await _update_job_progress(
+                session,
+                job_id,
+                progress=progress,
+                rows_processed=rows_seen,
+                rows_failed=rows_failed,
+            )
+
+    # final persist
+    progress = 100 if total_rows == 0 else int((rows_seen / max(total_rows, 1)) * 100)
+    await _update_job_progress(session, job_id, progress=progress, rows_processed=rows_seen, rows_failed=rows_failed)
+    return rows_seen, rows_failed
+
+
+async def _process_excel(path: Path, job_id: uuid.UUID, session: AsyncSession) -> tuple[int, int]:
+    wb = load_workbook(filename=str(path), read_only=True, data_only=True)
+    ws = wb.active
+
+    total_rows = max(0, (ws.max_row or 0) - 1)
+
+    rows_seen = 0
+    rows_failed = 0
+
+    header: list[str] = []
+    last_persist = time.monotonic()
+
+    rows_iter = ws.iter_rows(values_only=True)
+    try:
+        header_vals = next(rows_iter)
+        header = [str(c) if c is not None else "" for c in header_vals]
+    except StopIteration:
+        header = []
+
+    batch: list[dict[str, Any]] = []
+
+    for row_vals in rows_iter:
+        rows_seen += 1
+
+        job = await _get_job(session, job_id)
+        if not job or job.status == JobStatus.CANCELLED:
+            break
+
+        row_dict = {header[i] if i < len(header) else f"col_{i}": row_vals[i] for i in range(len(row_vals))}
+        batch.append(row_dict)
+
+        if len(batch) >= 10_000:
+            for i, item in enumerate(batch):
+                errs = _validate_row(item)
+                if errs:
+                    rows_failed += 1
+                    await _log_errors(session, job_id, rows_seen - len(batch) + i + 1, errs)
+            batch.clear()
+
+        now = time.monotonic()
+        if _should_persist(last_persist, now, settings.progress_update_interval_seconds):
+            last_persist = now
+            progress = 100 if total_rows == 0 else int((rows_seen / max(total_rows, 1)) * 100)
+            await _update_job_progress(session, job_id, progress=progress, rows_processed=rows_seen, rows_failed=rows_failed)
+
+    if batch:
+        for i, item in enumerate(batch):
+            errs = _validate_row(item)
+            if errs:
+                rows_failed += 1
+                await _log_errors(session, job_id, rows_seen - len(batch) + i + 1, errs)
+
+    progress = 100 if total_rows == 0 else int((rows_seen / max(total_rows, 1)) * 100)
+    await _update_job_progress(session, job_id, progress=progress, rows_processed=rows_seen, rows_failed=rows_failed)
+
+    wb.close()
+    return rows_seen, rows_failed
+
+
+async def _process_job_async(job_id_str: str) -> None:
+    engine = create_engine()
+    sessionmaker = create_sessionmaker(engine)
+
+    job_id = uuid.UUID(job_id_str)
+
+    async with sessionmaker() as session:
+        job = await _get_job(session, job_id)
+        if not job:
+            return
+
+        if job.status == JobStatus.CANCELLED:
+            return
+
+        await _update_job_progress(session, job_id, status=JobStatus.PROCESSING, started_at=_now(), error_message=None)
+
+        path = job_storage_path(job_id_str, job.filename)
+
+        try:
+            if job.file_type == "csv":
+                rows_processed, rows_failed = await _process_csv(path, job_id, session)
+            elif job.file_type in {"xlsx", "xls"}:
+                rows_processed, rows_failed = await _process_excel(path, job_id, session)
+            else:
+                raise ValueError("Unsupported file type")
+
+            refreshed = await _get_job(session, job_id)
+            if refreshed and refreshed.status == JobStatus.CANCELLED:
+                await _update_job_progress(session, job_id, completed_at=_now())
+                # cancelled jobs are retryable; webhook payload supports only COMPLETED|FAILED
+                if job.webhook_url:
+                    try:
+                        await deliver_webhook(
+                            job.webhook_url,
+                            job_id=job_id_str,
+                            status="FAILED",
+                            rows_processed=rows_processed,
+                            rows_failed=rows_failed,
+                            completed_at=_now(),
+                        )
+                    except Exception:
+                        pass
+                return
+
+            await _update_job_progress(session, job_id, status=JobStatus.COMPLETED, progress=100, completed_at=_now())
+
+            if job.webhook_url:
+                try:
+                    await deliver_webhook(
+                        job.webhook_url,
+                        job_id=job_id_str,
+                        status="COMPLETED",
+                        rows_processed=rows_processed,
+                        rows_failed=rows_failed,
+                        completed_at=_now(),
+                    )
+                except Exception:
+                    # Webhook failures should not fail the job processing.
+                    pass
+
+        except SoftTimeLimitExceeded:
+            await _update_job_progress(
+                session,
+                job_id,
+                status=JobStatus.FAILED,
+                error_message="Worker soft time limit exceeded; job can be retried",
+                completed_at=_now(),
+            )
+        except Exception as exc:  # noqa: BLE001
+            await _update_job_progress(
+                session,
+                job_id,
+                status=JobStatus.FAILED,
+                error_message=str(exc),
+                completed_at=_now(),
+            )
+            if job.webhook_url:
+                try:
+                    await deliver_webhook(
+                        job.webhook_url,
+                        job_id=job_id_str,
+                        status="FAILED",
+                        rows_processed=job.rows_processed,
+                        rows_failed=job.rows_failed,
+                        completed_at=_now(),
+                    )
+                except Exception:
+                    pass
+
+    await engine.dispose()
+
+
+@celery_app.task(name="process_job")
+def process_job(job_id: str) -> None:
+    asyncio.run(_process_job_async(job_id))
+
+
+async def check_redis_health(redis_url: str) -> bool:
+    try:
+        redis = Redis.from_url(redis_url)
+        await redis.ping()
+        await redis.aclose()
+        return True
+    except Exception:
+        return False
diff --git a/repository_after/webhook.py b/repository_after/webhook.py
new file mode 100644
index 00000000..97b44bd9
--- /dev/null
+++ b/repository_after/webhook.py
@@ -0,0 +1,41 @@
+from __future__ import annotations
+
+import asyncio
+from datetime import datetime, timezone
+
+import httpx
+
+
+async def deliver_webhook(
+    webhook_url: str,
+    *,
+    job_id: str,
+    status: str,
+    rows_processed: int,
+    rows_failed: int,
+    completed_at: datetime,
+) -> None:
+    payload = {
+        "job_id": job_id,
+        "status": status,
+        "rows_processed": rows_processed,
+        "rows_failed": rows_failed,
+        "completed_at": completed_at.astimezone(timezone.utc).isoformat(),
+    }
+
+    delays = [0.5, 1.0, 2.0]
+    last_exc: Exception | None = None
+
+    for attempt in range(3):
+        try:
+            async with httpx.AsyncClient(timeout=httpx.Timeout(2.0)) as client:
+                resp = await client.post(webhook_url, json=payload)
+                resp.raise_for_status()
+                return
+        except Exception as exc:  # noqa: BLE001
+            last_exc = exc
+            if attempt < 2:
+                await asyncio.sleep(delays[attempt])
+
+    if last_exc:
+        raise last_exc
