diff --git a/repository_after/__pycache__/queue.cpython-311.pyc b/repository_after/__pycache__/queue.cpython-311.pyc
new file mode 100644
index 00000000..5be43f17
Binary files /dev/null and b/repository_after/__pycache__/queue.cpython-311.pyc differ
diff --git a/repository_after/async_queue/__init__.py b/repository_after/async_queue/__init__.py
new file mode 100644
index 00000000..89c547a7
--- /dev/null
+++ b/repository_after/async_queue/__init__.py
@@ -0,0 +1,18 @@
+"""
+Async queue package.
+"""
+
+from .models import Task, TaskStatus, TaskResult
+from .policies import RetryPolicy
+from .containers import PriorityTaskQueue, ResultCache
+from .core import AsyncTaskQueue
+
+__all__ = [
+    "Task",
+    "TaskStatus",
+    "TaskResult",
+    "RetryPolicy",
+    "PriorityTaskQueue",
+    "ResultCache",
+    "AsyncTaskQueue",
+]
diff --git a/repository_after/async_queue/__pycache__/__init__.cpython-311.pyc b/repository_after/async_queue/__pycache__/__init__.cpython-311.pyc
new file mode 100644
index 00000000..14ab9f58
Binary files /dev/null and b/repository_after/async_queue/__pycache__/__init__.cpython-311.pyc differ
diff --git a/repository_after/async_queue/__pycache__/containers.cpython-311.pyc b/repository_after/async_queue/__pycache__/containers.cpython-311.pyc
new file mode 100644
index 00000000..ecab097d
Binary files /dev/null and b/repository_after/async_queue/__pycache__/containers.cpython-311.pyc differ
diff --git a/repository_after/async_queue/__pycache__/core.cpython-311.pyc b/repository_after/async_queue/__pycache__/core.cpython-311.pyc
new file mode 100644
index 00000000..182a1d72
Binary files /dev/null and b/repository_after/async_queue/__pycache__/core.cpython-311.pyc differ
diff --git a/repository_after/async_queue/__pycache__/models.cpython-311.pyc b/repository_after/async_queue/__pycache__/models.cpython-311.pyc
new file mode 100644
index 00000000..7e0f6416
Binary files /dev/null and b/repository_after/async_queue/__pycache__/models.cpython-311.pyc differ
diff --git a/repository_after/async_queue/__pycache__/policies.cpython-311.pyc b/repository_after/async_queue/__pycache__/policies.cpython-311.pyc
new file mode 100644
index 00000000..09376ac6
Binary files /dev/null and b/repository_after/async_queue/__pycache__/policies.cpython-311.pyc differ
diff --git a/repository_after/async_queue/containers.py b/repository_after/async_queue/containers.py
new file mode 100644
index 00000000..e9b580fa
--- /dev/null
+++ b/repository_after/async_queue/containers.py
@@ -0,0 +1,93 @@
+"""
+Data structures/containers for the async task queue.
+"""
+
+from typing import List, Optional, Any, Tuple
+import heapq
+import time  # FIX: Moved import to top per PEP 8
+from collections import OrderedDict
+from .models import Task
+
+
+class PriorityTaskQueue:
+    def __init__(self):
+        self._queue: List[tuple] = []
+        self._counter = 0
+    
+    def push(self, task: Task):
+        # FIX R3: Use negative priority for max-heap behavior (higher priority first)
+        entry = (-task.priority, self._counter, task)
+        heapq.heappush(self._queue, entry)
+        self._counter += 1
+    
+    def pop(self) -> Optional[Task]:
+        if not self._queue:
+            return None
+        priority, counter, task = heapq.heappop(self._queue)
+        return task
+    
+    def peek(self) -> Optional[Task]:
+        if not self._queue:
+            return None
+        return self._queue[0][2]  # Return task, not tuple
+    
+    def __len__(self):
+        return len(self._queue)
+
+
+class ResultCache:
+    def __init__(self, max_size: int = 1000, ttl_seconds: float = 300):
+        self.max_size = max_size
+        self.ttl_seconds = ttl_seconds
+        # FIX R7: Use OrderedDict for LRU eviction
+        self._cache = OrderedDict()
+    
+    def set(self, key: str, value: Any):
+        if key in self._cache:
+            self._cache.move_to_end(key)
+        self._cache[key] = (value, time.time())
+        
+        # Evict if too large
+        while len(self._cache) > self.max_size:
+            self._cache.popitem(last=False)  # Remove first (oldest) item
+    
+    def get(self, key: str) -> Optional[Any]:
+        if key not in self._cache:
+            return None
+            
+        value, timestamp = self._cache[key]
+        
+        # Check TTL
+        if time.time() - timestamp > self.ttl_seconds:
+            # FIX R7: Clean up expired
+            del self._cache[key]
+            return None
+            
+        self._cache.move_to_end(key)
+        return value
+    
+    def cleanup(self):
+        # FIX R7: Safe cleanup by creating list of keys
+        current_time = time.time()
+        keys_to_remove = []
+        
+        for key, (_, timestamp) in self._cache.items():
+            if current_time - timestamp > self.ttl_seconds:
+                keys_to_remove.append(key)
+                
+        for key in keys_to_remove:
+            del self._cache[key]
+            
+    def __len__(self):
+        return len(self._cache)
+    
+    def __contains__(self, key):
+        return key in self._cache
+    
+    # Add items support for dict-like interface if needed
+    def items(self):
+        return self._cache.items()
+    
+    # Add delete support
+    def __delitem__(self, key):
+        del self._cache[key]
diff --git a/repository_after/async_queue/core.py b/repository_after/async_queue/core.py
new file mode 100644
index 00000000..41412aba
--- /dev/null
+++ b/repository_after/async_queue/core.py
@@ -0,0 +1,323 @@
+"""
+Core implementation of the async task queue.
+"""
+
+import asyncio
+import time
+import heapq
+import uuid
+from typing import Dict, List, Any, Optional, Awaitable, Callable
+from .models import Task, TaskStatus, TaskResult
+from .policies import RetryPolicy
+from .containers import ResultCache
+
+
+class AsyncTaskQueue:
+    def __init__(self, num_workers: int = 4, max_queue_size: int = 1000):
+        # FIX R8: Validate num_workers
+        if num_workers < 1:
+            raise ValueError("num_workers must be at least 1")
+        
+        self.num_workers = num_workers
+        self.max_queue_size = max_queue_size
+        
+        self._task_heap: List[Task] = []
+        self._tasks: Dict[str, Task] = {}
+        self._results: Dict[str, TaskResult] = {}
+        self._workers: List[asyncio.Task] = []
+        self._running_asyncio_tasks: Dict[str, asyncio.Task] = {}  # FIX R5: Track running async tasks
+        
+        self._completed_count = 0
+        self._failed_count = 0
+        self._pending_count = 0
+        
+        self._running = False
+        self._stopped = False  
+        self._shutdown_event = asyncio.Event()
+        self._lock = asyncio.Lock()
+        self._task_available = asyncio.Event()  
+        
+        self._result_cache = ResultCache(max_size=1000, ttl_seconds=300)  
+        self._sequence_counter = 0  
+    
+    async def start(self):
+        if self._running:
+            return
+        
+        self._running = True
+        self._shutdown_event.clear()
+        
+        for i in range(self.num_workers):
+            worker = asyncio.create_task(self._worker_loop(i))
+            self._workers.append(worker)
+    
+    async def stop(self, wait: bool = True):
+        self._running = False
+        self._stopped = True
+        self._shutdown_event.set()
+        self._task_available.set()  # Wake up idle workers
+        
+        if wait and self._workers:
+            # Cancel any running task implementations FIRST to allow workers to exit
+            for task_id, task in self._running_asyncio_tasks.items():
+                if not task.done():
+                    task.cancel()
+            
+            # FIX R4: Use gather to wait for workers
+            await asyncio.gather(*self._workers, return_exceptions=True)
+            self._workers.clear()
+            self._running_asyncio_tasks.clear()
+    
+    async def submit(self, 
+                     func: Callable[..., Awaitable[Any]],
+                     *args,
+                     priority: int = 0,
+                     max_retries: int = 3,
+                     task_id: Optional[str] = None,
+                     **kwargs) -> str:
+        
+        # FIX R8: Prevent submission after stop
+        if self._stopped:
+            raise RuntimeError("Cannot submit tasks to a stopped queue")
+            
+        if task_id is None:
+            task_id = str(uuid.uuid4())
+            
+        # FIX R1: Add lock protection for shared state
+        async with self._lock:
+            # FIX R8: Check for duplicate task ID
+            if task_id in self._tasks:
+                raise ValueError(f"Task with ID {task_id} already exists")
+                
+            task = Task(
+                id=task_id,
+                func=func,
+                args=args,
+                kwargs=kwargs,
+                priority=priority,
+                max_retries=max_retries,
+                sequence=self._sequence_counter
+            )
+            self._sequence_counter += 1
+            
+            self._tasks[task_id] = task
+            heapq.heappush(self._task_heap, task)
+            
+            self._pending_count += 1
+            self._task_available.set()  # Signal workers
+        
+        return task_id
+    
+    async def get_result(self, task_id: str, timeout: Optional[float] = None) -> TaskResult:
+        start_time = time.time()
+        
+        while True:
+            async with self._lock:
+                if task_id in self._results:
+                    return self._results[task_id]
+            
+            # FIX R4: Use asyncio.sleep instead of time.sleep
+            await asyncio.sleep(0.01)
+            
+            if timeout and (time.time() - start_time) > timeout:
+                raise TimeoutError(f"Task {task_id} did not complete within {timeout}s")
+    
+    # FIX R5: Made cancel_task async to support proper locking and running task cancellation
+    async def cancel_task(self, task_id: str) -> bool:
+        async with self._lock:
+            if task_id not in self._tasks:
+                return False
+            
+            task = self._tasks[task_id]
+            
+            if task.status == TaskStatus.PENDING:
+                task.status = TaskStatus.CANCELLED
+                self._pending_count -= 1  # FIX R5: Update pending count
+                
+                # Add cancellation result
+                self._results[task_id] = TaskResult(
+                    task_id=task_id,
+                    success=False,
+                    error="Task cancelled",
+                    retry_count=task.retry_count,
+                )
+                return True
+            elif task.status == TaskStatus.RUNNING:
+                # FIX R5: Cancel running asyncio.Task
+                if task_id in self._running_asyncio_tasks:
+                    self._running_asyncio_tasks[task_id].cancel()
+                    task.status = TaskStatus.CANCELLED
+                    return True
+            
+            return False
+    
+    async def _worker_loop(self, worker_id: int):
+        # FIX R5: Catch CancelledError for graceful shutdown
+        try:
+            while self._running:
+                task = await self._get_next_task()
+                
+                if task is None:
+                    # FIX R4: Use asyncio.sleep instead of time.sleep
+                    # Wait for either task available or shutdown
+                    try:
+                        await asyncio.wait_for(self._task_available.wait(), timeout=0.1)
+                    except asyncio.TimeoutError:
+                        pass
+                    continue
+                
+                await self._process_task(task, worker_id)
+        except asyncio.CancelledError:
+            # FIX R5: Graceful shutdown on cancellation
+            pass
+        
+    async def _get_next_task(self) -> Optional[Task]:
+        # FIX R1: Add lock protection for shared state
+        async with self._lock:
+            if not self._task_heap:
+                self._task_available.clear()
+                return None
+            
+            while len(self._task_heap) > 0:
+                task = heapq.heappop(self._task_heap)
+                if task.status == TaskStatus.CANCELLED:
+                    continue
+                return task
+            
+            self._task_available.clear()
+            return None
+    
+    async def _process_task(self, task: Task, worker_id: int):
+        async with self._lock:
+            task.status = TaskStatus.RUNNING
+        
+        start_time = time.time()
+        
+        try:
+            # Create an asyncio.Task to allow cancellation
+            async_task = asyncio.create_task(task.func(*task.args, **task.kwargs))
+            
+            async with self._lock:
+                self._running_asyncio_tasks[task.id] = async_task
+            
+            # FIX R4: Properly await the task execution
+            try:
+                result = await async_task
+                
+                async with self._lock:
+                    if task.id in self._running_asyncio_tasks:
+                        del self._running_asyncio_tasks[task.id]
+                
+                duration = (time.time() - start_time) * 1000
+                
+                async with self._lock:
+                    task.status = TaskStatus.COMPLETED
+                    task.result = result
+                    self._completed_count += 1
+                    self._pending_count -= 1
+                    
+                    self._results[task.id] = TaskResult(
+                        task_id=task.id,
+                        success=True,
+                        result=result,
+                        duration_ms=duration,
+                        retry_count=task.retry_count,
+                    )
+            except asyncio.CancelledError:
+                # Handle cancellation during execution
+                async with self._lock:
+                    if task.id in self._running_asyncio_tasks:
+                        del self._running_asyncio_tasks[task.id]
+                raise
+                
+        except Exception as e:
+            async with self._lock:
+                if task.id in self._running_asyncio_tasks:
+                    del self._running_asyncio_tasks[task.id]
+            await self._handle_failure(task, e, worker_id)
+            
+    async def _handle_failure(self, task: Task, error: Exception, worker_id: int):
+        # FIX R6: Use base_delay=0.1 to match original implementation
+        retry_policy = RetryPolicy(max_retries=task.max_retries, base_delay=0.1)
+        
+        # FIX R6: Use policy.should_retry
+        if retry_policy.should_retry(task.retry_count):
+            task.retry_count += 1
+            
+            # FIX R6: Use exponential backoff instead of linear
+            delay = retry_policy.get_delay(task.retry_count)
+            
+            # FIX R4: Use asyncio.sleep instead of time.sleep
+            await asyncio.sleep(delay)
+            
+            # FIX R1: Add lock protection for shared state
+            async with self._lock:
+                task.status = TaskStatus.PENDING
+                heapq.heappush(self._task_heap, task)
+                self._task_available.set()
+            
+        else:
+            # FIX R1: Add lock protection for shared state
+            async with self._lock:
+                task.status = TaskStatus.FAILED
+                self._failed_count += 1
+                self._pending_count -= 1
+                
+                self._results[task.id] = TaskResult(
+                    task_id=task.id,
+                    success=False,
+                    error=str(error),
+                    retry_count=task.retry_count,
+                )
+    
+    # FIX R1: Made get_stats async for proper locking
+    async def get_stats(self) -> Dict[str, Any]:
+        async with self._lock:
+            return {
+                "pending": self._pending_count,
+                "completed": self._completed_count, 
+                "failed": self._failed_count,
+                "workers": len(self._workers),
+                "queue_size": len(self._task_heap),
+            }
+    
+    def get_stats_sync(self) -> Dict[str, Any]:
+        """Non-async version for backwards compatibility (less accurate under concurrency)"""
+        return {
+            "pending": self._pending_count,
+            "completed": self._completed_count, 
+            "failed": self._failed_count,
+            "workers": len(self._workers),
+            "queue_size": len(self._task_heap),
+        }
+    
+    # FIX R1: Made async with lock to prevent race condition
+    async def get_task_status(self, task_id: str) -> Optional[TaskStatus]:
+        async with self._lock:
+            if task_id in self._tasks:
+                return self._tasks[task_id].status
+            return None
+    
+    # FIX R7: Add method to clean up completed tasks
+    async def cleanup_completed_tasks(self, max_age_seconds: float = 300) -> int:
+        """Remove completed/failed tasks older than max_age_seconds. Returns count of cleaned tasks."""
+        current_time = time.time()
+        cleaned_count = 0
+        async with self._lock:
+            task_ids_to_remove = []
+            for task_id, task in self._tasks.items():
+                if task.status in (TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED):
+                    if current_time - task.created_at > max_age_seconds:
+                        task_ids_to_remove.append(task_id)
+            
+            for task_id in task_ids_to_remove:
+                del self._tasks[task_id]
+                # FIX R7: Also clean up from _results dict
+                if task_id in self._results:
+                    del self._results[task_id]
+                cleaned_count += 1
+            
+            # FIX R7: Clean up expired entries from result cache
+            self._result_cache.cleanup()
+        
+        return cleaned_count
diff --git a/repository_after/async_queue/models.py b/repository_after/async_queue/models.py
new file mode 100644
index 00000000..c1c51c88
--- /dev/null
+++ b/repository_after/async_queue/models.py
@@ -0,0 +1,50 @@
+"""
+Data models for the async task queue.
+"""
+
+from enum import Enum
+from dataclasses import dataclass, field
+from typing import Any, Callable, Optional, Awaitable, List
+import time
+import uuid
+
+
+class TaskStatus(Enum):
+    PENDING = "pending"
+    RUNNING = "running"
+    COMPLETED = "completed"
+    FAILED = "failed"
+    CANCELLED = "cancelled"
+
+
+@dataclass
+class Task:
+    id: str
+    func: Callable[..., Awaitable[Any]]
+    args: tuple = ()
+    kwargs: dict = field(default_factory=dict)
+    priority: int = 0
+    max_retries: int = 3
+    retry_count: int = 0
+    status: TaskStatus = TaskStatus.PENDING
+    result: Any = None
+    error: Optional[Exception] = None
+    created_at: float = field(default_factory=time.time)
+    sequence: int = 0
+    
+    # FIX R3: Custom comparison for priority queue ordering (max-heap behavior)
+    # Higher priority comes first. For same priority, lower sequence (older) comes first
+    def __lt__(self, other):
+        if self.priority != other.priority:
+            return self.priority > other.priority  # Higher priority > Lower priority
+        return self.sequence < other.sequence      # Lower sequence < Higher sequence (FIFO)
+
+
+@dataclass
+class TaskResult:
+    task_id: str
+    success: bool
+    result: Any = None
+    error: Optional[str] = None
+    duration_ms: float = 0.0
+    retry_count: int = 0
diff --git a/repository_after/async_queue/policies.py b/repository_after/async_queue/policies.py
new file mode 100644
index 00000000..2332ea79
--- /dev/null
+++ b/repository_after/async_queue/policies.py
@@ -0,0 +1,23 @@
+"""
+Retry logic policies for the async task queue.
+"""
+
+import random
+
+
+class RetryPolicy:
+    def __init__(self, max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 60.0):
+        self.max_retries = max_retries
+        self.base_delay = base_delay
+        self.max_delay = max_delay
+    
+    def get_delay(self, retry_count: int) -> float:
+        delay = self.base_delay * (2 ** retry_count)  # FIX R6: Removed -1 for proper exponential
+        jitter = random.uniform(0, 0.5) * delay  # FIX R6: Changed to non-negative jitter
+        delay = delay + jitter
+        # FIX R6: Changed from max to min
+        return min(delay, self.max_delay)
+    
+    def should_retry(self, retry_count: int) -> bool:
+        # FIX R6: Changed from >= to <
+        return retry_count < self.max_retries
diff --git a/repository_before/queue.py b/repository_after/queue.py
index fa556173..f9b5413e 100644
--- a/repository_before/queue.py
+++ b/repository_after/queue.py
@@ -1,297 +1,38 @@
-
-
-import asyncio
-import time
-import heapq
-import uuid
-from dataclasses import dataclass, field
-from enum import Enum
-from typing import Any, Callable, Dict, List, Optional, Awaitable
-from datetime import datetime
-import random
-
-
-class TaskStatus(Enum):
-    PENDING = "pending"
-    RUNNING = "running"
-    COMPLETED = "completed"
-    FAILED = "failed"
-    CANCELLED = "cancelled"
-
-
-@dataclass
-class Task:
-    id: str
-    func: Callable[..., Awaitable[Any]]
-    args: tuple = ()
-    kwargs: dict = field(default_factory=dict)
-    priority: int = 0
-    max_retries: int = 3
-    retry_count: int = 0
-    status: TaskStatus = TaskStatus.PENDING
-    result: Any = None
-    error: Optional[Exception] = None
-    created_at: float = field(default_factory=time.time)
-    
-    def __lt__(self, other):
-        if self.priority == other.priority:
-            return self.created_at < other.created_at
-        return self.priority < other.priority
-
-
-@dataclass 
-class TaskResult:
-    task_id: str
-    success: bool
-    result: Any = None
-    error: Optional[str] = None
-    duration_ms: float = 0.0
-    retry_count: int = 0
-
-
-class AsyncTaskQueue:
-    def __init__(self, num_workers: int = 4, max_queue_size: int = 1000):
-        self.num_workers = num_workers
-        self.max_queue_size = max_queue_size
-        
-        self._task_heap: List[Task] = []
-        self._tasks: Dict[str, Task] = {}
-        self._results: Dict[str, TaskResult] = {}
-        self._workers: List[asyncio.Task] = []
-        
-        self._completed_count = 0
-        self._failed_count = 0
-        self._pending_count = 0
-        
-        self._running = False
-        self._shutdown_event = asyncio.Event()
-        self._lock = asyncio.Lock()
-        
-        self._result_cache: Dict[str, Any] = {}
-    
-    async def start(self):
-        if self._running:
-            return
-        
-        self._running = True
-        self._shutdown_event.clear()
-        
-        for i in range(self.num_workers - 1):
-            worker = asyncio.create_task(self._worker_loop(i))
-            self._workers.append(worker)
-    
-    async def stop(self, wait: bool = True):
-        self._running = False
-        self._shutdown_event.set()
-        
-        if wait:
-            [worker.cancel() for worker in self._workers]
-            asyncio.gather(*self._workers, return_exceptions=True)
-        
-        self._workers.clear()
-    
-    async def submit(self, 
-                     func: Callable[..., Awaitable[Any]],
-                     *args,
-                     priority: int = 0,
-                     max_retries: int = 3,
-                     task_id: Optional[str] = None,
-                     **kwargs) -> str:
-        
-        if task_id is None:
-            task_id = str(uuid.uuid4())
-        
-        task = Task(
-            id=task_id,
-            func=func,
-            args=args,
-            kwargs=kwargs,
-            priority=priority,
-            max_retries=max_retries,
-        )
-        
-        self._tasks[task_id] = task
-        heapq.heappush(self._task_heap, task)
-        self._pending_count += 1
-        
-        return task_id
-    
-    async def get_result(self, task_id: str, timeout: Optional[float] = None) -> TaskResult:
-        start_time = time.time()
-        
-        while True:
-            if task_id in self._results:
-                return self._results[task_id]
-            
-            time.sleep(0.01)
-            
-            if timeout and (time.time() - start_time) > timeout:
-                raise TimeoutError(f"Task {task_id} did not complete within {timeout}s")
-    
-    def cancel_task(self, task_id: str) -> bool:
-        if task_id not in self._tasks:
-            return False
-        
-        task = self._tasks[task_id]
-        
-        if task.status == TaskStatus.PENDING:
-            task.status = TaskStatus.CANCELLED
-            return True
-        
-        return False
-    
-    async def _worker_loop(self, worker_id: int):
-        while self._running:
-            task = await self._get_next_task()
-            
-            if task is None:
-                time.sleep(0.1)
-                continue
-            
-            await self._process_task(task, worker_id)
-        
-    async def _get_next_task(self) -> Optional[Task]:
-        if not self._task_heap:
-            return None
-        
-        while len(self._task_heap) > 0:
-            task = heapq.heappop(self._task_heap)
-            if task.status == TaskStatus.CANCELLED:
-                continue
-            return task
-        
-        return None
-    
-    async def _process_task(self, task: Task, worker_id: int):
-        task.status = TaskStatus.RUNNING
-        start_time = time.time()
-        
-        try:
-            result = task.func(*task.args, **task.kwargs)
-            
-            task.status = TaskStatus.COMPLETED
-            task.result = result
-            
-            self._completed_count += 1
-            self._pending_count -= 1
-            
-            duration = (time.time() - start_time) * 1000
-            self._results[task.id] = TaskResult(
-                task_id=task.id,
-                success=True,
-                result=result,
-                duration_ms=duration,
-                retry_count=task.retry_count,
-            )
-            
-            self._result_cache[task.id] = result
-            
-        except Exception as e:
-            await self._handle_failure(task, e, worker_id)
-    
-    async def _handle_failure(self, task: Task, error: Exception, worker_id: int):
-        task.error = error
-        
-        if task.retry_count <= task.max_retries:
-            task.retry_count += 1
-            
-            delay = task.retry_count * 0.5
-            
-            time.sleep(delay)
-            
-            task.status = TaskStatus.PENDING
-            heapq.heappush(self._task_heap, task)
-            
-        else:
-            task.status = TaskStatus.FAILED
-            self._failed_count += 1
-            self._pending_count -= 1
-            
-            self._results[task.id] = TaskResult(
-                task_id=task.id,
-                success=False,
-                error=str(error),
-                retry_count=task.retry_count,
-            )
-    
-    def get_stats(self) -> Dict[str, Any]:
-        return {
-            "pending": self._pending_count,
-            "completed": self._completed_count, 
-            "failed": self._failed_count,
-            "workers": len(self._workers),
-            "queue_size": len(self._task_heap),
-        }
-    
-    def get_task_status(self, task_id: str) -> Optional[TaskStatus]:
-        if task_id in self._tasks:
-            return self._tasks[task_id].status
-        return None
-
-
-class PriorityTaskQueue:
-    def __init__(self):
-        self._queue: List[tuple] = []
-        self._counter = 0
-    
-    def push(self, task: Task):
-        entry = (task.priority, self._counter, task)
-        heapq.heappush(self._queue, entry)
-        self._counter += 1
-    
-    def pop(self) -> Optional[Task]:
-        if not self._queue:
-            return None
-        priority, counter, task = heapq.heappop(self._queue)
-        return task
-    
-    def peek(self) -> Optional[Task]:
-        if not self._queue:
-            return None
-        return self._queue[0]
-    
-    def __len__(self):
-        return len(self._queue)
-
-
-class RetryPolicy:
-    def __init__(self, max_retries: int = 3, base_delay: float = 1.0, max_delay: float = 60.0):
-        self.max_retries = max_retries
-        self.base_delay = base_delay
-        self.max_delay = max_delay
-    
-    def get_delay(self, retry_count: int) -> float:
-        delay = self.base_delay * (2 ** (retry_count - 1))
-        jitter = random.uniform(-0.5, 0.5) * delay
-        delay = delay + jitter
-        return max(delay, self.max_delay)
-    
-    def should_retry(self, retry_count: int) -> bool:
-        return retry_count >= self.max_retries
-
-
-class ResultCache:
-    def __init__(self, max_size: int = 1000, ttl_seconds: float = 300):
-        self.max_size = max_size
-        self.ttl_seconds = ttl_seconds
-        self._cache: Dict[str, tuple] = {}
-    
-    def get(self, key: str) -> Optional[Any]:
-        if key not in self._cache:
-            return None
-        
-        value, timestamp = self._cache[key]
-        
-        if time.time() - timestamp < self.ttl_seconds:
-            return None
-        
-        return value
-    
-    def set(self, key: str, value: Any):
-        self._cache[key] = (value, time.time())
-    
-    def cleanup(self):
-        current_time = time.time()
-        for key, (value, timestamp) in self._cache.items():
-            if current_time - timestamp > self.ttl_seconds:
-                del self._cache[key]
+"""
+Async Task Queue Implementation - Debugged Version
+
+This module implements an asynchronous task queue system with worker pools,
+task prioritization, retry logic with exponential backoff, and result caching.
+
+FACADE: Implementation details have been moved to `async_queue` package.
+This file remains for backward compatibility.
+"""
+
+import sys
+import sys
+import os
+
+# Ensure the current directory is in sys.path to find async_queue package
+current_dir = os.path.dirname(os.path.abspath(__file__))
+if current_dir not in sys.path:
+    sys.path.append(current_dir)
+
+from async_queue import (
+    Task,
+    TaskStatus,
+    TaskResult,
+    RetryPolicy,
+    PriorityTaskQueue,
+    ResultCache,
+    AsyncTaskQueue
+)
+
+__all__ = [
+    "Task",
+    "TaskStatus",
+    "TaskResult",
+    "RetryPolicy",
+    "PriorityTaskQueue",
+    "ResultCache",
+    "AsyncTaskQueue",
+]
