# PIU6UP - job queue system

**Category:** sft

## Overview
- Task ID: PIU6UP
- Title: job queue system
- Category: sft
- Repository: ep-eaglepoint-ai/bd_datasets_002
- Branch: piu6up-job-queue-system

## Requirements
- The implementation must define a Job class using the dataclasses module with the following fields: job_id (str, auto-generated UUID4 if not provided), job_type (str, required, non-empty), payload (Dict[str, Any], default empty dict), priority (int, range 1-10, default 5), scheduled_at (Optional[float], Unix timestamp, default None for immediate execution), created_at (float, auto-set to current time), max_retries (int, default 3, minimum 0), retry_count (int, default 0, tracks current attempt number), timeout_seconds (Optional[float], default None for no timeout), status (JobStatus enum, default PENDING), result (Optional[Any], stores successful execution result), error (Optional[str], stores last error message), and metadata (Dict[str, Any], default empty dict for user tracking data). The Job class must implement __lt__ for heap comparison using the ordering rules: higher priority first, earlier scheduled_at second, earlier created_at third. A class method Job.create() must provide a convenient factory that validates all inputs and raises ValueError for invalid job_type, priority out of range, or negative max_retries. The JobStatus enum must define PENDING, SCHEDULED, RUNNING, COMPLETED, FAILED, and DEAD states.
- The JobQueue class must maintain jobs in a heap structure using the heapq module, with a companion dictionary mapping job_id to Job objects for O(1) lookup and status updates. The enqueue(job: Job) method must add jobs to the heap and dictionary, returning the job_id. The dequeue() method must return the highest-priority job that is ready for execution (scheduled_at is None or scheduled_at <= current_time and status is PENDING), skipping over jobs that are scheduled for future execution or in non-pending states. If no jobs are ready, dequeue must return None rather than blocking. The peek() method must return the next ready job without removing it. The get_job(job_id: str) method must return the job or raise KeyError. The update_status(job_id: str, status: JobStatus) method must update a job's status in O(1) time. The size() property must return total job count, and ready_count() must return count of jobs ready for immediate execution. The class must handle the complexity that heap operations are O(log n) but finding specific jobs in a heap is O(n) by using the dictionary for lookups and marking removed jobs as "tombstones" that are skipped during dequeue rather than restructuring the heap. A periodic cleanup method must compact the heap by removing tombstones when they exceed 25% of heap size.
- The Worker class must be a threading.Thread subclass that continuously polls its assigned queue for jobs, executes them using registered handlers, and manages job lifecycle state transitions. The constructor must accept the queue to poll, a dictionary mapping job_type to handler callables, and a poll_interval (default 0.1 seconds). The run() method must loop until a stop flag is set: dequeue a job, set status to RUNNING, execute the handler with job.payload as argument, set status to COMPLETED with result stored if successful, or handle failures appropriately. Handler execution must be wrapped in a timeout mechanism using threading.Timer or concurrent.futures with timeout that raises TimeoutError if job.timeout_seconds is exceeded. Any exception during execution (including timeout) must be caught, the error message stored in job.error, and the job passed to the failure handling logic. The worker must implement graceful shutdown: when stop() is called, finish the current job if running, then exit the loop. Workers must emit events or call callbacks for job state transitions to enable monitoring. Each worker must have a unique worker_id for debugging.
- When a job execution fails, the worker must determine if retries remain by comparing job.retry_count < job.max_retries. If retries remain, increment retry_count, calculate the next execution time using exponential backoff: scheduled_at = current_time + base_delay * (2 ** retry_count) + random_jitter, where base_delay is configurable (default 1.0 second) and jitter is random uniform between 0 and 0.5 * calculated_delay to prevent thundering herd. Set status back to PENDING and re-enqueue the job. The job must retain its original priority so retried jobs don't starve behind lower-priority new jobs. If no retries remain (retry_count >= max_retries), set status to DEAD and move the job to the dead letter queue. The dead letter queue must be a separate JobQueue instance accessible via the manager, storing failed jobs indefinitely until manually acknowledged or replayed. A replay_dead_letter(job_id: str) method must reset a dead job's retry_count to 0, status to PENDING, and re-enqueue it to the original queue. The implementation must log all retry attempts and dead letter movements with job_id, job_type, retry_count, and error message.
- The JobQueueManager class must be the primary interface for the job queue system. The constructor must accept a configuration dictionary specifying queue names and their settings (default_priority, max_workers, etc.). The create_queue(name: str, **config) method must create and register a new JobQueue with the given configuration. The register_handler(job_type: str, handler: Callable, queue: str = "default") method must associate a handler function with a job type and target queue. The submit(job_type: str, payload: dict, **kwargs) method must create a Job with the given parameters, determine the target queue from the registered handler, enqueue it, and return the job_id. The start_workers(queue: str = None) method must spawn Worker threads for the specified queue (or all queues if None), respecting the max_workers configuration. The stop_workers(queue: str = None, graceful: bool = True) method must signal workers to stop, optionally waiting for current jobs to complete. The manager must provide get_stats() returning a dictionary with total_submitted, total_completed, total_failed, total_dead, jobs_per_queue, and workers_per_queue. The manager must implement the context manager protocol for clean startup and shutdown.
- The JobQueue class must use threading.Lock to protect all heap and dictionary operations, ensuring that concurrent enqueue and dequeue calls from multiple threads do not corrupt data structures or cause race conditions. The lock must be held for the minimum duration necessary: acquire before reading/modifying shared state, release immediately after. Use context managers (with self._lock:) for exception safety. The Worker class must acquire the queue's lock when dequeueing jobs and updating status. The handler registry and worker list in JobQueueManager must be protected by separate locks to allow concurrent job submissions while workers are being started/stopped. The implementation must avoid deadlocks by always acquiring locks in a consistent order (queue lock before worker lock) and never holding locks while executing user-provided handler functions. A stress test spawning 10 producer threads each submitting 1000 jobs and 5 consumer workers must complete without deadlocks, lost jobs, or duplicate processing, with final job counts matching expected values.
- Jobs must support an optional depends_on field containing a list of job_ids that must complete successfully before this job can execute. When a job with dependencies is submitted, it must be stored but not considered ready for execution until all dependencies have COMPLETED status. The dequeue method must check dependencies: if any dependency is PENDING, SCHEDULED, or RUNNING, skip this job; if any dependency is FAILED or DEAD, this job must also be marked FAILED with an appropriate error message; only if all dependencies are COMPLETED is the job ready. The implementation must handle circular dependency detection at submission time by traversing the dependency graph and raising ValueError if a cycle would be created. When a job completes successfully, the system must efficiently identify and potentially wake up dependent jobs rather than scanning all jobs on every dequeue. This can be implemented using a reverse dependency index mapping job_id to list of dependent job_ids. The Job class must include a depends_on: List[str] field defaulting to empty list, and the dependency checking logic must be clearly documented.
- The library must support event callbacks for monitoring job lifecycle: on_job_submitted(job), on_job_started(job, worker_id), on_job_completed(job, result), on_job_failed(job, error), on_job_retried(job, next_scheduled_at), and on_job_dead(job). The JobQueueManager must accept an optional event_handler object implementing these methods, with a default no-op implementation. Jobs must be serializable to and from JSON for persistence: Job.to_dict() must return a dictionary representation with all fields (converting enums to strings and timestamps to floats), and Job.from_dict(data) must reconstruct a Job instance. The JobQueue must support export_jobs() returning a list of all job dictionaries and import_jobs(jobs_data) for restoring state. This enables periodic snapshots for crash recovery. The serialization must handle all field types including the payload dictionary which may contain nested structures. A round-trip test (export then import) must produce jobs that compare equal to the originals.

## Metadata
- Programming Languages: Python
- Frameworks: (none)
- Libraries: (none)
- Databases: (none)
- Tools: (none)
- Best Practices: (none)
- Performance Metrics: (none)
- Security Standards: (none)

## Structure
- repository_before/: baseline code (`__init__.py`)
- repository_after/: optimized code (`__init__.py`)
- tests/: test suite (`__init__.py`)
- evaluation/: evaluation scripts (`evaluation.py`)
- instances/: sample/problem instances (JSON)
- patches/: patches for diffing
- trajectory/: notes or write-up (Markdown)

## Quick start
- Run tests locally: `python -m pytest -q tests`
- With Docker: `docker compose up --build --abort-on-container-exit`
- Add dependencies to `requirements.txt`

## Notes
- Keep commits focused and small.
- Open a PR when ready for review.
