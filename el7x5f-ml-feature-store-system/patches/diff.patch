diff --git a/repository_before/__init__.py b/repository_after/__init__.py
index e69de29b..414d8cce 100644
--- a/repository_before/__init__.py
+++ b/repository_after/__init__.py
@@ -0,0 +1,6 @@
+"""Repository-after implementation.
+
+Exports the feature store public API from `repository_after.feature_store`.
+"""
+
+from .feature_store import *  # noqa: F401,F403
diff --git a/repository_after/__pycache__/__init__.cpython-311.pyc b/repository_after/__pycache__/__init__.cpython-311.pyc
new file mode 100644
index 00000000..e08bb473
Binary files /dev/null and b/repository_after/__pycache__/__init__.cpython-311.pyc differ
diff --git a/repository_after/app.py b/repository_after/app.py
new file mode 100644
index 00000000..1b115cf3
--- /dev/null
+++ b/repository_after/app.py
@@ -0,0 +1,15 @@
+import os
+
+from feature_store.api import AppSettings, create_app
+
+
+def _env(name: str, default: str) -> str:
+    return os.environ.get(name, default)
+
+
+settings = AppSettings(
+    database_url=_env("DATABASE_URL", "postgresql+psycopg2://postgres:postgres@postgres:5432/feature_store"),
+    redis_url=_env("REDIS_URL", "redis://redis:6379/0"),
+)
+
+app = create_app(settings)
diff --git a/repository_after/feature_store/__init__.py b/repository_after/feature_store/__init__.py
new file mode 100644
index 00000000..f3372bf2
--- /dev/null
+++ b/repository_after/feature_store/__init__.py
@@ -0,0 +1,38 @@
+"""Feature store library package.
+
+This package provides:
+- Declarative DSL for features
+- SQLAlchemy-backed registry (PostgreSQL)
+- Online serving (Redis)
+- Training data generation (point-in-time correct joins)
+- FastAPI app for discovery and serving
+
+Optional integrations (Spark/Faust/Great Expectations) are designed to be
+importable without requiring those heavy dependencies at runtime.
+"""
+
+from .dsl import (
+    Feature,
+    FeatureMetadata,
+    FeatureSource,
+    PythonTransform,
+    SQLTransform,
+    feature,
+)
+from .registry import FeatureRegistry, RegistrySettings
+from .serving import OnlineStore, RedisOnlineStore
+from .pit_join import point_in_time_join_pandas
+
+__all__ = [
+    "Feature",
+    "FeatureMetadata",
+    "FeatureSource",
+    "PythonTransform",
+    "SQLTransform",
+    "feature",
+    "FeatureRegistry",
+    "RegistrySettings",
+    "OnlineStore",
+    "RedisOnlineStore",
+    "point_in_time_join_pandas",
+]
diff --git a/repository_after/feature_store/__pycache__/__init__.cpython-311.pyc b/repository_after/feature_store/__pycache__/__init__.cpython-311.pyc
new file mode 100644
index 00000000..7b3b9c39
Binary files /dev/null and b/repository_after/feature_store/__pycache__/__init__.cpython-311.pyc differ
diff --git a/repository_after/feature_store/__pycache__/api.cpython-311.pyc b/repository_after/feature_store/__pycache__/api.cpython-311.pyc
new file mode 100644
index 00000000..17b46a27
Binary files /dev/null and b/repository_after/feature_store/__pycache__/api.cpython-311.pyc differ
diff --git a/repository_after/feature_store/__pycache__/db.cpython-311.pyc b/repository_after/feature_store/__pycache__/db.cpython-311.pyc
new file mode 100644
index 00000000..5067ab86
Binary files /dev/null and b/repository_after/feature_store/__pycache__/db.cpython-311.pyc differ
diff --git a/repository_after/feature_store/__pycache__/dsl.cpython-311.pyc b/repository_after/feature_store/__pycache__/dsl.cpython-311.pyc
new file mode 100644
index 00000000..63a77c3f
Binary files /dev/null and b/repository_after/feature_store/__pycache__/dsl.cpython-311.pyc differ
diff --git a/repository_after/feature_store/__pycache__/models.cpython-311.pyc b/repository_after/feature_store/__pycache__/models.cpython-311.pyc
new file mode 100644
index 00000000..5b94a5fe
Binary files /dev/null and b/repository_after/feature_store/__pycache__/models.cpython-311.pyc differ
diff --git a/repository_after/feature_store/__pycache__/pit_join.cpython-311.pyc b/repository_after/feature_store/__pycache__/pit_join.cpython-311.pyc
new file mode 100644
index 00000000..1fe3ed58
Binary files /dev/null and b/repository_after/feature_store/__pycache__/pit_join.cpython-311.pyc differ
diff --git a/repository_after/feature_store/__pycache__/registry.cpython-311.pyc b/repository_after/feature_store/__pycache__/registry.cpython-311.pyc
new file mode 100644
index 00000000..bccc96bb
Binary files /dev/null and b/repository_after/feature_store/__pycache__/registry.cpython-311.pyc differ
diff --git a/repository_after/feature_store/__pycache__/serving.cpython-311.pyc b/repository_after/feature_store/__pycache__/serving.cpython-311.pyc
new file mode 100644
index 00000000..55dcab33
Binary files /dev/null and b/repository_after/feature_store/__pycache__/serving.cpython-311.pyc differ
diff --git a/repository_after/feature_store/api.py b/repository_after/feature_store/api.py
new file mode 100644
index 00000000..ebc91f71
--- /dev/null
+++ b/repository_after/feature_store/api.py
@@ -0,0 +1,123 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Sequence
+
+from fastapi import Depends, FastAPI, HTTPException
+from fastapi.responses import HTMLResponse
+from pydantic import BaseModel
+
+from .registry import FeatureRegistry, RegistrySettings
+from .serving import RedisOnlineStore, RedisOnlineStoreSettings
+
+
+class FeatureSummary(BaseModel):
+    name: str
+    version: str
+    description: str
+    owner: str
+    tags: List[str]
+    entity_keys: List[str]
+    event_timestamp: str
+    source: Dict[str, Any]
+    transform: Dict[str, Any]
+    depends_on: List[str]
+
+
+class GetOnlineRequest(BaseModel):
+    feature_set: str
+    entity_key: str
+    feature_names: List[str]
+    max_age_seconds: Optional[int] = None
+
+
+class GetOnlineBatchRequest(BaseModel):
+    feature_set: str
+    entity_keys: List[str]
+    feature_names: List[str]
+    max_age_seconds: Optional[int] = None
+
+
+@dataclass(frozen=True)
+class AppSettings:
+    database_url: str
+    redis_url: str
+
+
+def create_app(settings: AppSettings) -> FastAPI:
+    registry = FeatureRegistry(RegistrySettings(database_url=settings.database_url))
+    registry.create_schema()
+    online = RedisOnlineStore(RedisOnlineStoreSettings(redis_url=settings.redis_url))
+
+    app = FastAPI(title="Feature Store", version="0.1.0")
+
+    def get_registry() -> FeatureRegistry:
+        return registry
+
+    def get_online() -> RedisOnlineStore:
+        return online
+
+    @app.get("/health")
+    def health() -> Dict[str, str]:
+        return {"status": "ok"}
+
+    @app.get("/features", response_model=List[FeatureSummary])
+    def list_features(reg: FeatureRegistry = Depends(get_registry)):
+        return reg.list_features()
+
+    @app.get("/features/{name}", response_model=FeatureSummary)
+    def get_feature(name: str, version: str = "v1", reg: FeatureRegistry = Depends(get_registry)):
+        try:
+            f = reg.get(name=name, version=version)
+        except Exception as e:
+            raise HTTPException(status_code=404, detail="Feature not found")
+
+        # Return as summary dict
+        d = reg.list_features()
+        for it in d:
+            if it["name"] == name and it["version"] == version:
+                return it
+        raise HTTPException(status_code=404, detail="Feature not found")
+
+    @app.post("/online/get")
+    def online_get(req: GetOnlineRequest, store: RedisOnlineStore = Depends(get_online)):
+        return store.get_features(
+            feature_set=req.feature_set,
+            entity_key=req.entity_key,
+            feature_names=req.feature_names,
+            defaults=None,
+            max_age_seconds=req.max_age_seconds,
+        )
+
+    @app.post("/online/get_batch")
+    def online_get_batch(req: GetOnlineBatchRequest, store: RedisOnlineStore = Depends(get_online)):
+        return store.get_features_batch(
+            feature_set=req.feature_set,
+            entity_keys=req.entity_keys,
+            feature_names=req.feature_names,
+            defaults=None,
+            max_age_seconds=req.max_age_seconds,
+        )
+
+    @app.get("/ui", response_class=HTMLResponse)
+    def web_ui(reg: FeatureRegistry = Depends(get_registry)):
+        features = reg.list_features()
+        rows = "".join(
+            f"<tr><td>{f['name']}</td><td>{f['version']}</td><td>{f['owner']}</td><td>{f['description']}</td></tr>"
+            for f in features
+        )
+        html = f"""
+        <html>
+          <head><title>Feature Store UI</title></head>
+          <body>
+            <h1>Feature Registry</h1>
+            <table border='1' cellpadding='6' cellspacing='0'>
+              <tr><th>Name</th><th>Version</th><th>Owner</th><th>Description</th></tr>
+              {rows}
+            </table>
+          </body>
+        </html>
+        """
+        return HTMLResponse(content=html)
+
+    return app
diff --git a/repository_after/feature_store/catalog_hooks.py b/repository_after/feature_store/catalog_hooks.py
new file mode 100644
index 00000000..10ab166f
--- /dev/null
+++ b/repository_after/feature_store/catalog_hooks.py
@@ -0,0 +1,13 @@
+from __future__ import annotations
+
+from typing import Any, Dict, Protocol
+
+
+class DataCatalogHook(Protocol):
+    def publish_feature(self, feature_definition: Dict[str, Any]) -> None:
+        ...
+
+
+class NoopCatalogHook:
+    def publish_feature(self, feature_definition: Dict[str, Any]) -> None:
+        return
diff --git a/repository_after/feature_store/db.py b/repository_after/feature_store/db.py
new file mode 100644
index 00000000..132fe91d
--- /dev/null
+++ b/repository_after/feature_store/db.py
@@ -0,0 +1,19 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Optional
+
+from sqlalchemy import create_engine
+from sqlalchemy.engine import Engine
+from sqlalchemy.orm import sessionmaker
+
+
+@dataclass(frozen=True)
+class DatabaseSettings:
+    database_url: str
+
+
+def create_engine_and_session_factory(settings: DatabaseSettings) -> tuple[Engine, sessionmaker]:
+    engine = create_engine(settings.database_url, pool_pre_ping=True)
+    SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False)
+    return engine, SessionLocal
diff --git a/repository_after/feature_store/dsl.py b/repository_after/feature_store/dsl.py
new file mode 100644
index 00000000..1d228fdb
--- /dev/null
+++ b/repository_after/feature_store/dsl.py
@@ -0,0 +1,101 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple, Union
+
+
+@dataclass(frozen=True)
+class FeatureMetadata:
+    description: str
+    owner: str
+    tags: Tuple[str, ...] = ()
+    version: str = "v1"
+
+
+@dataclass(frozen=True)
+class FeatureSource:
+    """Binding to a data source.
+
+    In production you can back this by:
+    - a SQL table/view (offline)
+    - a Kafka topic (streaming)
+
+    This class is intentionally lightweight; richer connectors can wrap it.
+    """
+
+    name: str
+    kind: str  # "sql" | "kafka" | "custom"
+    identifier: str  # table name, topic name, etc.
+
+
+class Transform:
+    kind: str
+
+
+@dataclass(frozen=True)
+class SQLTransform(Transform):
+    sql: str
+    kind: str = "sql"
+
+
+@dataclass(frozen=True)
+class PythonTransform(Transform):
+    func: Callable[[Any], Any]
+    kind: str = "python"
+
+
+@dataclass
+class Feature:
+    """Declarative feature definition.
+
+    A Feature may depend on other Features. Dependencies are used for lineage.
+
+    Minimal fields to support training-serving consistency:
+    - entity_keys: the join keys (e.g. user_id)
+    - event_timestamp: column name used for point-in-time joins
+    """
+
+    name: str
+    entity_keys: Tuple[str, ...]
+    event_timestamp: str
+    source: FeatureSource
+    transform: Union[SQLTransform, PythonTransform]
+    metadata: FeatureMetadata
+    depends_on: Tuple[str, ...] = ()
+    default_value: Optional[Any] = None
+
+    def dependency_set(self) -> Set[str]:
+        return set(self.depends_on)
+
+
+def feature(
+    *,
+    name: str,
+    entity_keys: Sequence[str],
+    event_timestamp: str,
+    source: FeatureSource,
+    transform: Union[SQLTransform, PythonTransform],
+    description: str,
+    owner: str,
+    tags: Sequence[str] = (),
+    version: str = "v1",
+    depends_on: Sequence[str] = (),
+    default_value: Optional[Any] = None,
+) -> Feature:
+    """Convenience constructor for Feature definitions."""
+
+    return Feature(
+        name=name,
+        entity_keys=tuple(entity_keys),
+        event_timestamp=event_timestamp,
+        source=source,
+        transform=transform,
+        metadata=FeatureMetadata(
+            description=description,
+            owner=owner,
+            tags=tuple(tags),
+            version=version,
+        ),
+        depends_on=tuple(depends_on),
+        default_value=default_value,
+    )
diff --git a/repository_after/feature_store/examples.py b/repository_after/feature_store/examples.py
new file mode 100644
index 00000000..4a161c9e
--- /dev/null
+++ b/repository_after/feature_store/examples.py
@@ -0,0 +1,37 @@
+from __future__ import annotations
+
+from .dsl import FeatureSource, PythonTransform, SQLTransform, feature
+
+
+def example_features():
+    users = FeatureSource(name="users", kind="sql", identifier="public.users")
+
+    f_age = feature(
+        name="user_age",
+        entity_keys=["user_id"],
+        event_timestamp="event_time",
+        source=users,
+        transform=SQLTransform(sql="SELECT user_id, event_time, age AS user_age FROM users"),
+        description="User age at event time",
+        owner="ml-platform",
+        tags=["demographics"],
+        version="v1",
+        depends_on=[],
+        default_value=None,
+    )
+
+    f_is_adult = feature(
+        name="user_is_adult",
+        entity_keys=["user_id"],
+        event_timestamp="event_time",
+        source=users,
+        transform=PythonTransform(func=lambda row: int(row["user_age"]) >= 18),
+        description="Derived feature: adult flag",
+        owner="ml-platform",
+        tags=["demographics", "derived"],
+        version="v1",
+        depends_on=["user_age"],
+        default_value=0,
+    )
+
+    return [f_age, f_is_adult]
diff --git a/repository_after/feature_store/models.py b/repository_after/feature_store/models.py
new file mode 100644
index 00000000..a9650bb4
--- /dev/null
+++ b/repository_after/feature_store/models.py
@@ -0,0 +1,54 @@
+from __future__ import annotations
+
+import json
+from datetime import datetime
+from typing import Any, Dict, Optional
+
+from sqlalchemy import JSON, DateTime, Integer, String, Text, UniqueConstraint
+from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
+
+
+class Base(DeclarativeBase):
+    pass
+
+
+class FeatureDefinitionModel(Base):
+    __tablename__ = "feature_definitions"
+    __table_args__ = (UniqueConstraint("name", "version", name="uq_feature_name_version"),)
+
+    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
+    name: Mapped[str] = mapped_column(String(255), nullable=False)
+    version: Mapped[str] = mapped_column(String(64), nullable=False)
+
+    description: Mapped[str] = mapped_column(Text, nullable=False)
+    owner: Mapped[str] = mapped_column(String(255), nullable=False)
+    tags: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=False, default=dict)
+
+    entity_keys: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=False)
+    event_timestamp: Mapped[str] = mapped_column(String(255), nullable=False)
+
+    source: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=False)
+    transform: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=False)
+    depends_on: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=False, default=dict)
+
+    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, default=datetime.utcnow)
+    updated_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, default=datetime.utcnow)
+
+
+class FeatureStatsModel(Base):
+    __tablename__ = "feature_stats"
+
+    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
+    feature_name: Mapped[str] = mapped_column(String(255), nullable=False, index=True)
+    feature_version: Mapped[str] = mapped_column(String(64), nullable=False)
+    stats: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=False)
+    computed_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, default=datetime.utcnow)
+
+
+class FeatureLineageEdgeModel(Base):
+    __tablename__ = "feature_lineage_edges"
+
+    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
+    upstream: Mapped[str] = mapped_column(String(255), nullable=False, index=True)
+    downstream: Mapped[str] = mapped_column(String(255), nullable=False, index=True)
+    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, default=datetime.utcnow)
diff --git a/repository_after/feature_store/pit_join.py b/repository_after/feature_store/pit_join.py
new file mode 100644
index 00000000..e778c0f4
--- /dev/null
+++ b/repository_after/feature_store/pit_join.py
@@ -0,0 +1,58 @@
+from __future__ import annotations
+
+from typing import List, Sequence
+
+import pandas as pd
+
+
+def point_in_time_join_pandas(
+    *,
+    labels: pd.DataFrame,
+    features: pd.DataFrame,
+    entity_keys: Sequence[str],
+    label_time_col: str,
+    feature_time_col: str,
+    feature_cols: Sequence[str],
+) -> pd.DataFrame:
+    """Point-in-time correct join (pandas).
+
+    For each label row, picks the latest feature row for the same entity where
+    feature_time <= label_time.
+
+    This prevents leakage by construction.
+    """
+
+    if labels.empty:
+        return labels.copy()
+
+    for col in list(entity_keys) + [label_time_col]:
+        if col not in labels.columns:
+            raise KeyError(f"labels missing column: {col}")
+    for col in list(entity_keys) + [feature_time_col] + list(feature_cols):
+        if col not in features.columns:
+            raise KeyError(f"features missing column: {col}")
+
+    left = labels.copy()
+    right = features.copy()
+
+    left[label_time_col] = pd.to_datetime(left[label_time_col], utc=True)
+    right[feature_time_col] = pd.to_datetime(right[feature_time_col], utc=True)
+
+    # Sort for merge_asof
+    left = left.sort_values(list(entity_keys) + [label_time_col])
+    right = right.sort_values(list(entity_keys) + [feature_time_col])
+
+    joined = left
+    # merge_asof supports a single "by" list for exact match on entity keys
+    joined = pd.merge_asof(
+        joined,
+        right[list(entity_keys) + [feature_time_col] + list(feature_cols)],
+        left_on=label_time_col,
+        right_on=feature_time_col,
+        by=list(entity_keys),
+        direction="backward",
+        allow_exact_matches=True,
+    )
+
+    # Drop the feature timestamp unless user explicitly needs it
+    return joined.drop(columns=[feature_time_col])
diff --git a/repository_after/feature_store/processing_batch.py b/repository_after/feature_store/processing_batch.py
new file mode 100644
index 00000000..8727726d
--- /dev/null
+++ b/repository_after/feature_store/processing_batch.py
@@ -0,0 +1,47 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict, Optional, Sequence
+
+from .dsl import Feature
+
+
+@dataclass(frozen=True)
+class SparkBatchSettings:
+    watermark_delay: str = "1 day"
+
+
+class SparkBatchProcessor:
+    """Batch feature computation using PySpark.
+
+    This module is intentionally import-light; PySpark is optional.
+    """
+
+    def __init__(self, settings: SparkBatchSettings):
+        self._settings = settings
+
+    def compute(self, *, spark, feature: Feature, source_df, watermark_col: Optional[str] = None):
+        """Compute a feature DataFrame.
+
+        - Supports incremental processing by applying watermarks when watermark_col is provided.
+        - Supports backfills by running on historical source_df.
+
+        Note: execution details depend on your source bindings; this is a minimal reference implementation.
+        """
+
+        try:
+            import pyspark.sql.functions as F
+        except Exception as e:  # pragma: no cover
+            raise RuntimeError("PySpark is required for SparkBatchProcessor") from e
+
+        df = source_df
+        if watermark_col is not None:
+            df = df.withWatermark(watermark_col, self._settings.watermark_delay)
+
+        # SQL transforms are executed in the pipeline that provides a view.
+        if getattr(feature.transform, "kind", None) == "sql":
+            # The caller can register df as a temp view and run feature.transform.sql.
+            return df
+
+        # Python transforms for Spark should be expressed as Spark SQL expressions/UDFs.
+        return df
diff --git a/repository_after/feature_store/processing_stream.py b/repository_after/feature_store/processing_stream.py
new file mode 100644
index 00000000..53f1ef25
--- /dev/null
+++ b/repository_after/feature_store/processing_stream.py
@@ -0,0 +1,28 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict, Optional, Sequence
+
+
+@dataclass(frozen=True)
+class StreamSettings:
+    app_id: str = "feature-store"
+
+
+class FaustStreamProcessor:
+    """Kafka + Faust streaming feature computation.
+
+    This is a scaffolding layer suitable for production extension.
+    The project keeps Faust optional so the library can be installed without Kafka.
+    """
+
+    def __init__(self, settings: StreamSettings):
+        self._settings = settings
+
+    def build_app(self):
+        try:
+            import faust
+        except Exception as e:  # pragma: no cover
+            raise RuntimeError("Faust is required for FaustStreamProcessor") from e
+
+        return faust.App(self._settings.app_id)
diff --git a/repository_after/feature_store/py.typed b/repository_after/feature_store/py.typed
new file mode 100644
index 00000000..e69de29b
diff --git a/repository_after/feature_store/registry.py b/repository_after/feature_store/registry.py
new file mode 100644
index 00000000..07e45839
--- /dev/null
+++ b/repository_after/feature_store/registry.py
@@ -0,0 +1,192 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple
+
+import networkx as nx
+from sqlalchemy import delete, select
+from sqlalchemy.orm import Session
+
+from .db import DatabaseSettings, create_engine_and_session_factory
+from .dsl import Feature, FeatureMetadata, FeatureSource, PythonTransform, SQLTransform
+from .models import Base, FeatureDefinitionModel, FeatureLineageEdgeModel, FeatureStatsModel
+
+
+@dataclass(frozen=True)
+class RegistrySettings:
+    database_url: str
+
+
+class FeatureRegistry:
+    """SQLAlchemy-backed registry.
+
+    Stores feature definitions (schemas, metadata) + lineage edges.
+    """
+
+    def __init__(self, settings: RegistrySettings):
+        self._settings = settings
+        self._engine, self._SessionLocal = create_engine_and_session_factory(
+            DatabaseSettings(database_url=settings.database_url)
+        )
+
+    @property
+    def engine(self):
+        return self._engine
+
+    def create_schema(self) -> None:
+        Base.metadata.create_all(self._engine)
+
+    def drop_schema(self) -> None:
+        Base.metadata.drop_all(self._engine)
+
+    def _session(self) -> Session:
+        return self._SessionLocal()
+
+    def register(self, feature: Feature) -> None:
+        """Upsert a feature definition and its lineage edges."""
+
+        with self._session() as session:
+            existing = session.execute(
+                select(FeatureDefinitionModel).where(
+                    FeatureDefinitionModel.name == feature.name,
+                    FeatureDefinitionModel.version == feature.metadata.version,
+                )
+            ).scalar_one_or_none()
+
+            row = FeatureDefinitionModel(
+                name=feature.name,
+                version=feature.metadata.version,
+                description=feature.metadata.description,
+                owner=feature.metadata.owner,
+                tags={"tags": list(feature.metadata.tags)},
+                entity_keys={"entity_keys": list(feature.entity_keys)},
+                event_timestamp=feature.event_timestamp,
+                source={
+                    "name": feature.source.name,
+                    "kind": feature.source.kind,
+                    "identifier": feature.source.identifier,
+                },
+                transform=self._serialize_transform(feature),
+                depends_on={"depends_on": list(feature.depends_on)},
+            )
+
+            if existing is None:
+                session.add(row)
+            else:
+                existing.description = row.description
+                existing.owner = row.owner
+                existing.tags = row.tags
+                existing.entity_keys = row.entity_keys
+                existing.event_timestamp = row.event_timestamp
+                existing.source = row.source
+                existing.transform = row.transform
+                existing.depends_on = row.depends_on
+
+                session.execute(
+                    delete(FeatureLineageEdgeModel).where(
+                        FeatureLineageEdgeModel.downstream == feature.name
+                    )
+                )
+
+            for upstream in feature.depends_on:
+                session.add(FeatureLineageEdgeModel(upstream=upstream, downstream=feature.name))
+
+            session.commit()
+
+    def get(self, name: str, version: Optional[str] = None) -> Feature:
+        version = version or "v1"
+        with self._session() as session:
+            row = session.execute(
+                select(FeatureDefinitionModel).where(
+                    FeatureDefinitionModel.name == name,
+                    FeatureDefinitionModel.version == version,
+                )
+            ).scalar_one()
+
+            return self._deserialize_feature(row)
+
+    def list_features(self) -> List[Dict[str, Any]]:
+        with self._session() as session:
+            rows = session.execute(select(FeatureDefinitionModel)).scalars().all()
+            return [
+                {
+                    "name": r.name,
+                    "version": r.version,
+                    "description": r.description,
+                    "owner": r.owner,
+                    "tags": r.tags.get("tags", []),
+                    "entity_keys": r.entity_keys.get("entity_keys", []),
+                    "event_timestamp": r.event_timestamp,
+                    "source": r.source,
+                    "transform": r.transform,
+                    "depends_on": r.depends_on.get("depends_on", []),
+                }
+                for r in rows
+            ]
+
+    def lineage_graph(self) -> nx.DiGraph:
+        g = nx.DiGraph()
+        with self._session() as session:
+            edges = session.execute(select(FeatureLineageEdgeModel)).scalars().all()
+            for e in edges:
+                g.add_edge(e.upstream, e.downstream)
+        return g
+
+    def record_stats(self, feature_name: str, feature_version: str, stats: Dict[str, Any]) -> None:
+        with self._session() as session:
+            session.add(
+                FeatureStatsModel(feature_name=feature_name, feature_version=feature_version, stats=stats)
+            )
+            session.commit()
+
+    def latest_stats(self, feature_name: str, feature_version: str) -> Optional[Dict[str, Any]]:
+        with self._session() as session:
+            row = session.execute(
+                select(FeatureStatsModel)
+                .where(
+                    FeatureStatsModel.feature_name == feature_name,
+                    FeatureStatsModel.feature_version == feature_version,
+                )
+                .order_by(FeatureStatsModel.computed_at.desc())
+            ).scalars().first()
+            return None if row is None else row.stats
+
+    def _serialize_transform(self, feature: Feature) -> Dict[str, Any]:
+        t = feature.transform
+        if isinstance(t, SQLTransform):
+            return {"kind": "sql", "sql": t.sql}
+        if isinstance(t, PythonTransform):
+            # Do not attempt to pickle code for registry; store a reference.
+            return {"kind": "python", "callable": getattr(t.func, "__name__", "<callable>")}
+        raise TypeError(f"Unsupported transform type: {type(t)}")
+
+    def _deserialize_feature(self, row: FeatureDefinitionModel) -> Feature:
+        source = FeatureSource(
+            name=row.source["name"],
+            kind=row.source["kind"],
+            identifier=row.source["identifier"],
+        )
+        transform = row.transform
+        if transform.get("kind") == "sql":
+            tx = SQLTransform(sql=transform["sql"])
+        else:
+            # For python transforms, the registry stores only a reference.
+            # Execution is handled by pipelines that bind actual callables.
+            tx = SQLTransform(sql="-- python transform reference stored in registry")
+
+        md = FeatureMetadata(
+            description=row.description,
+            owner=row.owner,
+            tags=tuple(row.tags.get("tags", [])),
+            version=row.version,
+        )
+
+        return Feature(
+            name=row.name,
+            entity_keys=tuple(row.entity_keys.get("entity_keys", [])),
+            event_timestamp=row.event_timestamp,
+            source=source,
+            transform=tx,
+            metadata=md,
+            depends_on=tuple(row.depends_on.get("depends_on", [])),
+        )
diff --git a/repository_after/feature_store/sdk_pandas.py b/repository_after/feature_store/sdk_pandas.py
new file mode 100644
index 00000000..3f5f0715
--- /dev/null
+++ b/repository_after/feature_store/sdk_pandas.py
@@ -0,0 +1,35 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict, List, Mapping, Optional, Sequence
+
+import pandas as pd
+
+from .serving import OnlineStore
+
+
+@dataclass(frozen=True)
+class PandasFeatureFetcher:
+    online_store: OnlineStore
+    feature_set: str
+
+    def fetch(
+        self,
+        *,
+        entities: pd.DataFrame,
+        entity_key_col: str,
+        feature_names: Sequence[str],
+        defaults: Optional[Mapping[str, Any]] = None,
+        max_age_seconds: Optional[int] = None,
+    ) -> pd.DataFrame:
+        rows: List[Dict[str, Any]] = []
+        for ek in entities[entity_key_col].astype(str).tolist():
+            feats = self.online_store.get_features(
+                feature_set=self.feature_set,
+                entity_key=ek,
+                feature_names=feature_names,
+                defaults=defaults,
+                max_age_seconds=max_age_seconds,
+            )
+            rows.append({entity_key_col: ek, **feats})
+        return entities.merge(pd.DataFrame(rows), on=entity_key_col, how="left")
diff --git a/repository_after/feature_store/sdk_sklearn.py b/repository_after/feature_store/sdk_sklearn.py
new file mode 100644
index 00000000..3caeadcb
--- /dev/null
+++ b/repository_after/feature_store/sdk_sklearn.py
@@ -0,0 +1,38 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Mapping, Optional, Sequence
+
+import numpy as np
+import pandas as pd
+
+from .sdk_pandas import PandasFeatureFetcher
+
+
+@dataclass
+class FeatureStoreTransformer:
+    """scikit-learn compatible transformer interface.
+
+    This transformer fetches online features given an entity key column.
+    """
+
+    fetcher: PandasFeatureFetcher
+    entity_key_col: str
+    feature_names: Sequence[str]
+    defaults: Optional[Mapping[str, Any]] = None
+    max_age_seconds: Optional[int] = None
+
+    def fit(self, X, y=None):
+        return self
+
+    def transform(self, X):
+        if not isinstance(X, pd.DataFrame):
+            X = pd.DataFrame(X)
+        df = self.fetcher.fetch(
+            entities=X,
+            entity_key_col=self.entity_key_col,
+            feature_names=self.feature_names,
+            defaults=self.defaults,
+            max_age_seconds=self.max_age_seconds,
+        )
+        return df[self.feature_names].to_numpy()
diff --git a/repository_after/feature_store/sdk_torch.py b/repository_after/feature_store/sdk_torch.py
new file mode 100644
index 00000000..b825172b
--- /dev/null
+++ b/repository_after/feature_store/sdk_torch.py
@@ -0,0 +1,44 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Mapping, Optional, Sequence
+
+try:
+    import torch
+    from torch.utils.data import Dataset
+except Exception:  # pragma: no cover
+    torch = None
+    Dataset = object
+
+import pandas as pd
+
+from .sdk_pandas import PandasFeatureFetcher
+
+
+@dataclass
+class FeatureStoreDataset(Dataset):
+    """PyTorch Dataset that fetches features on-the-fly."""
+
+    entities: pd.DataFrame
+    entity_key_col: str
+    feature_names: Sequence[str]
+    fetcher: PandasFeatureFetcher
+    defaults: Optional[Mapping[str, Any]] = None
+    max_age_seconds: Optional[int] = None
+
+    def __len__(self) -> int:
+        return len(self.entities)
+
+    def __getitem__(self, idx: int):
+        row = self.entities.iloc[[idx]]
+        df = self.fetcher.fetch(
+            entities=row,
+            entity_key_col=self.entity_key_col,
+            feature_names=self.feature_names,
+            defaults=self.defaults,
+            max_age_seconds=self.max_age_seconds,
+        )
+        values = df[self.feature_names].iloc[0].to_list()
+        if torch is None:  # pragma: no cover
+            return values
+        return torch.tensor(values)
diff --git a/repository_after/feature_store/serving.py b/repository_after/feature_store/serving.py
new file mode 100644
index 00000000..497251b9
--- /dev/null
+++ b/repository_after/feature_store/serving.py
@@ -0,0 +1,142 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from datetime import datetime, timezone
+from typing import Any, Dict, Iterable, List, Mapping, Optional, Protocol, Sequence, Tuple
+
+
+class OnlineStore(Protocol):
+    def write_features(
+        self,
+        *,
+        feature_set: str,
+        entity_key: str,
+        values: Mapping[str, Any],
+        event_time: datetime,
+    ) -> None:
+        ...
+
+    def get_features(
+        self,
+        *,
+        feature_set: str,
+        entity_key: str,
+        feature_names: Sequence[str],
+        defaults: Optional[Mapping[str, Any]] = None,
+        max_age_seconds: Optional[int] = None,
+    ) -> Dict[str, Any]:
+        ...
+
+    def get_features_batch(
+        self,
+        *,
+        feature_set: str,
+        entity_keys: Sequence[str],
+        feature_names: Sequence[str],
+        defaults: Optional[Mapping[str, Any]] = None,
+        max_age_seconds: Optional[int] = None,
+    ) -> Dict[str, Dict[str, Any]]:
+        ...
+
+
+@dataclass(frozen=True)
+class RedisOnlineStoreSettings:
+    redis_url: str
+    key_prefix: str = "fs"
+
+
+class RedisOnlineStore:
+    """Low-latency Redis-backed online store.
+
+    Uses Redis hashes + a per-entity freshness timestamp.
+    This works everywhere without requiring RedisTimeSeries module.
+
+    You can swap to RedisTimeSeries later by implementing a different writer/reader.
+    """
+
+    def __init__(self, settings: RedisOnlineStoreSettings):
+        import redis
+
+        self._settings = settings
+        self._client = redis.Redis.from_url(settings.redis_url, decode_responses=True)
+
+    def _entity_hash_key(self, feature_set: str, entity_key: str) -> str:
+        return f"{self._settings.key_prefix}:{feature_set}:{entity_key}:values"
+
+    def _entity_meta_key(self, feature_set: str, entity_key: str) -> str:
+        return f"{self._settings.key_prefix}:{feature_set}:{entity_key}:meta"
+
+    def write_features(
+        self,
+        *,
+        feature_set: str,
+        entity_key: str,
+        values: Mapping[str, Any],
+        event_time: datetime,
+    ) -> None:
+        if event_time.tzinfo is None:
+            event_time = event_time.replace(tzinfo=timezone.utc)
+        ts = int(event_time.timestamp())
+
+        hkey = self._entity_hash_key(feature_set, entity_key)
+        mkey = self._entity_meta_key(feature_set, entity_key)
+
+        pipe = self._client.pipeline()
+        # Values are stored as strings by redis-py; keep it simple.
+        pipe.hset(hkey, mapping={k: "" if v is None else str(v) for k, v in values.items()})
+        pipe.hset(mkey, mapping={"event_time": str(ts)})
+        pipe.execute()
+
+    def get_features(
+        self,
+        *,
+        feature_set: str,
+        entity_key: str,
+        feature_names: Sequence[str],
+        defaults: Optional[Mapping[str, Any]] = None,
+        max_age_seconds: Optional[int] = None,
+    ) -> Dict[str, Any]:
+        defaults = defaults or {}
+
+        hkey = self._entity_hash_key(feature_set, entity_key)
+        mkey = self._entity_meta_key(feature_set, entity_key)
+
+        pipe = self._client.pipeline()
+        pipe.hmget(hkey, list(feature_names))
+        pipe.hget(mkey, "event_time")
+        values, event_time_s = pipe.execute()
+
+        now_ts = int(datetime.now(tz=timezone.utc).timestamp())
+        if max_age_seconds is not None and event_time_s is not None:
+            age = now_ts - int(event_time_s)
+            if age > max_age_seconds:
+                # staleness => return defaults
+                return {name: defaults.get(name) for name in feature_names}
+
+        out: Dict[str, Any] = {}
+        for name, raw in zip(feature_names, values):
+            if raw is None:
+                out[name] = defaults.get(name)
+            else:
+                out[name] = raw
+        return out
+
+    def get_features_batch(
+        self,
+        *,
+        feature_set: str,
+        entity_keys: Sequence[str],
+        feature_names: Sequence[str],
+        defaults: Optional[Mapping[str, Any]] = None,
+        max_age_seconds: Optional[int] = None,
+    ) -> Dict[str, Dict[str, Any]]:
+        return {
+            ek: self.get_features(
+                feature_set=feature_set,
+                entity_key=ek,
+                feature_names=feature_names,
+                defaults=defaults,
+                max_age_seconds=max_age_seconds,
+            )
+            for ek in entity_keys
+        }
diff --git a/repository_after/feature_store/validation.py b/repository_after/feature_store/validation.py
new file mode 100644
index 00000000..05e9bf6b
--- /dev/null
+++ b/repository_after/feature_store/validation.py
@@ -0,0 +1,32 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Any, Dict, Optional
+
+
+@dataclass(frozen=True)
+class ValidationResult:
+    success: bool
+    details: Dict[str, Any]
+
+
+class GreatExpectationsValidator:
+    """Great Expectations integration (optional).
+
+    The intent is:
+    - automatic profiling
+    - schema + statistical validation
+    - drift checks between training/serving
+
+    This is kept minimal for portability.
+    """
+
+    def __init__(self):
+        try:
+            import great_expectations as ge  # noqa: F401
+        except Exception as e:  # pragma: no cover
+            raise RuntimeError("great_expectations is required for validation") from e
+
+    def validate_dataframe(self, df, expectation_suite=None) -> ValidationResult:
+        # Minimal stub; real implementation should build a DataContext.
+        return ValidationResult(success=True, details={"note": "validation stub"})
