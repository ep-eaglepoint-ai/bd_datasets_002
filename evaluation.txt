Evaluation Guide (Trainer & Evaluator
Standard)
1. Purpose
This guide denes a standard for implementing evaluation/evaluation.py and producing a
consistent evaluation report comparing:
repository_before/
repository_after/
The goal is:
Comparable evaluations
Machine-readable reports
Minimal freedom only where task logic requires it
2. Required Repository Structure
repository_before/
repository_after/
tests/
evaluation/
evaluation.py
reports/
3. What evaluation.py Must Do
Every evaluator must:
. Collect run metadata
. Run correctness tests on before and after
. Optionally collect task metrics
. Compare results
. Write a report JSON
. Exit with the correct status code
4. Required Python Contract
evaluation/evaluation.py must expose:
run_evaluation() -> dict
main() -> int
And end with:
if __name__ == "__main__":
sys.exit(main())
5. Success Rule (Default)
Unless explicitly overridden by the task:
success = after.tests.passed == true
Metrics do not decide success by default.
6. Standard Report Structure ( report.json )
This is the single canonical reference.
{
"run_id": "uuid",
"started_at": "ISO-8601",
"finished_at": "ISO-8601",
"duration_seconds": 0.0,
"environment": {
"python_version": "3.x",
"platform": "os-arch"
},
"before": {
"tests": {
"passed": false,
"return_code": 1,
"output": "pytest output (truncated)"
},
"metrics": {}
},
"after": {
"tests": {
"passed": true,
"return_code": 0,
"output": "pytest output (truncated)"
},
"metrics": {}
},
"comparison": {
"passed_gate": true,
"improvement_summary": "short human-readable summary"
},
"success": true,
"error": null
}
7. Metrics Standard
Rules
Metrics are optional
If present:
JSON-serializable
Numbers or booleans only
Same measurement logic for before & after
Sample Metric Keys
Category KeyCategory Key
Performance avg_time_ms
Performance p95_time_ms
Stability failures
Stability failure_rate
Concurrency deadlocks
Throughput ops_per_second
Data rows_processed
Quality warnings
Example Metrics Block
"metrics": {
"avg_time_ms": 310.8,
"p95_time_ms": 520.4,
"failures": 0,
"failure_rate": 0.0,
"deadlocks": 0,
"ops_per_second": 128.7,
"rows_processed": 10000,
"warnings": 1
}
8. Recommended Python Libraries (Approved)
Trainers are encouraged to use only standard or well-known libraries.
Core (always allowed)
import os
import sys
import time
import json
import uuid
import platform
import subprocess
from pathlib import Path
from datetime import datetime
Testing
pytest # via subprocess
Performance / timing
time
time.perf_counter
statistics # mean, median
Concurrency (if needed)
threading
concurrent.futures
multiprocessing
Optional (only if task requires)
psutil # CPU / memory metrics
sqlite3
sqlalchemy
ðŸš« Avoid:
Randomized metrics without xed seeds
External services
Heavy frameworks
Network calls
9. Sample evaluation.py Skeleton (Standard)
This is the baseline template trainers should start from.
#!/usr/bin/env python3
import sys
import json
import time
import uuid
import platform
import subprocess
from pathlib import Path
from datetime import datetime
ROOT = Path(__file__).resolve().parent.parent
REPORTS = ROOT / "evaluation" / "reports"
def environment_info():
return {
"python_version": platform.python_version(),
"platform": platform.platform()
}
def run_tests():
try:
proc = subprocess.run(
["pytest", "tests", "-q"],
cwd=ROOT,
capture_output=True,
text=True,
timeout=120
)
return {
"passed": proc.returncode == 0,
"return_code": proc.returncode,
"output": (proc.stdout + proc.stderr)[:8000]
}
except subprocess.TimeoutExpired:
return {
"passed": False,
"return_code": -1,
"output": "pytest timeout"
}
def run_metrics(repo_path: Path):
# Optional â€“ trainers implement if needed
return {}
def evaluate(repo_name: str):
repo_path = ROOT / repo_name
tests = run_tests()
metrics = run_metrics(repo_path)
return {
"tests": tests,
"metrics": metrics
}
def run_evaluation():
run_id = str(uuid.uuid4())
start = datetime.utcnow()
before = evaluate("repository_before")
after = evaluate("repository_after")
comparison = {
"passed_gate": after["tests"]["passed"],
"improvement_summary": "After implementation passed correctness c
}
end = datetime.utcnow()
return {
"run_id": run_id,
"started_at": start.isoformat() + "Z",
"finished_at": end.isoformat() + "Z",
"duration_seconds": (end - start).total_seconds(),
"environment": environment_info(),
"before": before,
"after": after,
"comparison": comparison,
"success": comparison["passed_gate"],
"error": None
}
10. Error Handling Rules
If evaluation crashes:
success = false
error must contain message
Other elds may be null
11. Trainer Checklist
Before submitting:
Same tests run for before & after
Metrics are numeric only
Report matches schema
latest.json written
Exit code reects success
12. Design Principles
Clarity over cleverness
Deterministic results
Minimal schema drift
One report = one truth
def main():
REPORTS.mkdir(parents=True, exist_ok=True)
report = run_evaluation()
path = report.json should be included inside the reports folder, and it must also be added to .gitignore.evaluation/â””â”€â”€ reports/Â Â Â Â â””â”€â”€ YYYY-MM-DD/Â Â Â Â Â Â Â Â â””â”€â”€ HH-MM-SS/Â Â Â Â Â Â Â Â Â Â Â Â â””â”€â”€ report.jso
path.write_text(json.dumps(report, indent=2))
print(f"Report written to {path}")
return 0 if report["success"] else 1
if __name__ == "__main__":