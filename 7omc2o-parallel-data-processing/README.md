# 7OMC2O - parallel-data-processing

**Category:** rl

## Overview
- Task ID: 7OMC2O
- Title: parallel-data-processing
- Category: rl
- Repository: ep-eaglepoint-ai/bd_datasets_002
- Branch: 7omc2o-parallel-data-processing

## Requirements
- The optimized implementation must completely eliminate all instances of creating Process objects for individual work items or small batches. Every parallel operation must use either multiprocessing.Pool with the map, imap, imap_unordered, starmap, or apply_async methods, or concurrent.futures.ProcessPoolExecutor with the map or submit methods. The pool must be created once at processor initialization with a worker count equal to os.cpu_count() or a user-specified value, and reused across all operations rather than creating new pools per method call. Process pool context managers (with statements) must be used to ensure proper cleanup. The process_data_spawn_per_item method must be replaced with a single pool.map call that distributes all items across workers, and the parallel_sum_spawn_many method must use pool.map with proper chunking to achieve O(n/p) time complexity where p is the number of workers, compared to the current O(n) process creation overhead that dominates computation.
- The implementation must calculate optimal chunk sizes based on the formula max(1, total_items // (num_workers * 4)) to ensure each worker receives enough work to amortize serialization and deserialization costs while maintaining load balance. No worker should ever receive fewer than 100 items unless the total dataset is smaller than 100 * num_workers. The chunk size calculation must be encapsulated in a reusable method that considers both item count and estimated per-item computation time. For operations with variable per-item cost such as prime checking, the implementation must use imap_unordered with explicit chunksize parameter to handle load imbalance. The current pattern of spawning processes for chunk sizes of 5-10 items must be completely eliminated, as process creation and IPC overhead (typically 1-10ms) vastly exceeds the computation time for such small batches. Performance testing must demonstrate that processing 100,000 items takes less than 2 seconds with 4 workers, compared to the current implementation which would take several minutes due to process spawn overhead.
- The process_with_file_based_ipc method and all associated serialize_to_file and deserialize_from_file calls must be removed entirely. For numeric data arrays, the implementation must use multiprocessing.Array with ctypes to create shared memory regions that workers can access without serialization overhead. For complex data structures that cannot use shared memory, multiprocessing.Queue with batched operations must be used, where workers accumulate results locally and perform a single queue.put of the entire batch rather than per-item puts. The temp_files list and cleanup method become unnecessary and must be removed. The implementation must not perform any disk I/O for inter-process communication under normal operation. Memory-mapped files using mmap may be used only for datasets exceeding available RAM, but must not be the default communication mechanism. Performance testing must show that IPC overhead accounts for less than 10% of total execution time for typical workloads.
- The parallel_matrix_multiply method currently spawns a separate process for each element of the result matrix, resulting in O(n²) process creations for an n×n matrix multiplication. This must be replaced with a numpy-based implementation that performs the actual multiplication using np.matmul or the @ operator, which leverages optimized BLAS libraries for O(n³) computation with excellent cache utilization. For matrices large enough to benefit from parallelization (dimension > 1000), the implementation must partition the computation by rows of the result matrix, with each worker computing a contiguous block of rows using numpy operations. The workers must receive views or copies of the relevant matrix portions and return their computed row blocks. For matrices smaller than the parallelization threshold, the implementation must use single-threaded numpy multiplication as the process overhead would exceed computation time. The implementation must handle matrices of any size without spawning more processes than the pool size, and performance testing must show 100x100 matrix multiplication completing in under 10 milliseconds compared to the current multi-second execution time.
- The find_primes_worker method currently uses trial division with O(n√n) complexity per range segment, checking divisibility by all integers from 2 to n-1 for each candidate. This must be replaced with the Sieve of Eratosthenes algorithm that marks composite numbers by iterating through multiples of each prime. For parallel execution, the implementation must use the segmented sieve approach where each worker processes a contiguous range after receiving the list of base primes up to √limit from the main process. The base primes must be computed once using a sequential sieve up to √limit, then broadcast to all workers. Each worker creates a boolean array for its segment, marks composites using the base primes, and returns the indices of unmarked positions. The final aggregation must use list concatenation or itertools.chain rather than repeated append operations. The bubble sort for final ordering must be replaced with sorted() or list.sort(). Performance testing must demonstrate finding all primes up to 1,000,000 in under 500 milliseconds with 4 workers, compared to the current implementation which takes tens of seconds due to inefficient primality testing.
- The current pipeline implementation passes items one at a time through queues between stages, incurring serialization overhead for each item. The optimized implementation must batch items at each stage, collecting results into local lists and transferring batches of 100-1000 items at once through queues. Queues must be bounded using the maxsize parameter to provide backpressure and prevent memory exhaustion when stages have different throughput rates. The pipeline must support configurable batch sizes and queue depths. The sentinel value pattern (putting None to signal completion) must be retained but applied at the batch level rather than item level. For pipelines where stages have significantly different processing times, the implementation should consider stage replication where slower stages run multiple worker processes consuming from the same input queue. The Manager and shared list pattern must be replaced with a final output queue that the main process drains after all stages complete. Performance testing must show that pipeline throughput is within 20% of the throughput of the slowest individual stage when processing 10,000+ items.
- The map_reduce_word_count_mapper method builds strings character by character using concatenation within a loop, resulting in O(n²) complexity for processing n characters. This must be replaced with list comprehension or generator expression combined with str.join, such as ''.join(c.lower() for c in word if c.isalpha()). The word splitting and chunk creation in map_reduce_word_count uses string concatenation in a loop and must be replaced with list slicing on the split result followed by ' '.join() for chunk reconstruction, or better yet, index-based partitioning that avoids creating intermediate chunk strings entirely by passing start and end indices to workers who then process text[start:end].split(). The generate_test_text function must use a list comprehension with join rather than concatenation. All dictionary updates for word counting must use dict.get(key, 0) + 1 or collections.Counter for automatic aggregation. The final reduction phase should use collections.Counter with the update method or sum(counters, Counter()) pattern for efficient merging of partial counts. Performance testing must show word counting of 1 million words completing in under 2 seconds.
- The optimized processor must implement the context manager protocol (__enter__ and __exit__ methods) to ensure process pools are properly shut down even when exceptions occur. The __exit__ method must call pool.terminate() followed by pool.join() to forcefully clean up workers if normal shutdown fails. All public methods must validate input parameters and raise informative exceptions for invalid inputs such as empty data lists, non-positive worker counts, or incompatible matrix dimensions. Workers must catch and re-raise exceptions with full tracebacks so that errors in worker processes are properly reported to the main process rather than silently failing or hanging. The implementation must handle keyboard interrupts gracefully by terminating worker processes and cleaning up resources. A timeout parameter must be available for long-running operations, with proper cleanup if the timeout expires. The API must remain backward-compatible with the original method signatures, accepting the same parameters and returning results in the same format, so that existing code using the unoptimized processor can switch to the optimized version without modification. Performance metrics including total time, items per second, and CPU utilization should be available through an optional verbose mode or separate timing method.

## Metadata
- Programming Languages: Python
- Frameworks: (none)
- Libraries: (none)
- Databases: (none)
- Tools: (none)
- Best Practices: (none)
- Performance Metrics: (none)
- Security Standards: (none)

## Structure
- repository_before/: baseline code (`__init__.py`)
- repository_after/: optimized code (`__init__.py`)
- tests/: test suite (`__init__.py`)
- evaluation/: evaluation scripts (`evaluation.py`)
- instances/: sample/problem instances (JSON)
- patches/: patches for diffing
- trajectory/: notes or write-up (Markdown)

## Quick start
- Run tests locally: `python -m pytest -q tests`
- With Docker: `docker compose up --build --abort-on-container-exit`
- Add dependencies to `requirements.txt`

## Notes
- Keep commits focused and small.
- Open a PR when ready for review.
