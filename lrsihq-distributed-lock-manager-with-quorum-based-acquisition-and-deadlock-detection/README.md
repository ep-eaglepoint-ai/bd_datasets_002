# LRSIHQ - Distributed Lock Manager with Quorum-Based Acquisition and Deadlock Detection

**Category:** sft

## Overview
- Task ID: LRSIHQ
- Title: Distributed Lock Manager with Quorum-Based Acquisition and Deadlock Detection
- Category: sft
- Repository: ep-eaglepoint-ai/bd_datasets_002
- Branch: lrsihq-distributed-lock-manager-with-quorum-based-acquisition-and-deadlock-detection

## Requirements
- 1. Distributed Lock Acquisition with Quorum - The acquire_lock(resource_id, client_id, mode, timeout_ms) function must return a lock_token on success or None on failure. - Lock acquisition must use Lamport timestamps for total ordering: each request tagged with {client_id, timestamp, sequence_number}. - Quorum-based acquisition: A lock is granted only when a majority of nodes (quorum_size) acknowledge the request. - When 5 nodes exist with quorum_size=3, a lock is granted when 3 out of 5 nodes respond with GRANT. - Lock token format: {resource_id, client_id, mode, timestamp, version, expires_at, signature}. - Acquisition latency must be p99 < 50ms (time from request to receiving quorum responses). - The system must support 10,000 concurrent lock requests without degradation. - When a client requests a lock that is already held, the request must be queued in FIFO order based on Lamport timestamp. - Lock acquisition must be atomic: either all quorum nodes grant the lock or none do (no partial grants).   2. Lock Release and Automatic Timeout - The release_lock(resource_id, lock_token) function must release the lock and propagate the release to all nodes within 100ms. - Locks must have an automatic timeout (default 30 seconds) to prevent indefinite holding due to client crashes. - Heartbeat mechanism: Clients must send heartbeat(lock_token) every 10 seconds to extend the lock expiration. - When a heartbeat is missed for 15 seconds (1.5x heartbeat interval), the lock must be marked as expired. - Timeout detection must occur within 5 seconds of expiry: a background thread checks for expired locks every 5 seconds. - When a client crashes while holding a lock, the lock must be automatically released after timeout (30 seconds). - Force release: An admin can call force_release(resource_id, client_id) to immediately release a lock held by a specific client. - Release propagation: When a lock is released, all nodes in the cluster must be notified within 100ms.
- 3. Deadlock Detection and Resolution - The system must build a wait-for graph: an edge from client A to client B means A is waiting for a lock held by B. - Deadlock detection must run every 1 second: traverse the wait-for graph to detect cycles using depth-first search (DFS). - When a cycle is detected (e.g., A waits for B, B waits for C, C waits for A), the system must abort the youngest transaction (highest timestamp). - Deadlock detection latency must be < 2 seconds (time from deadlock occurrence to detection and resolution). - Prevention strategy: If a client cannot acquire a lock within timeout_ms (default 30 seconds), the request must be aborted. - When a deadlock is detected, the system must log: {timestamp, cycle: [client_A, client_B, client_C], aborted_client: client_C}. - The aborted client must receive an error: "Deadlock detected: your transaction was aborted to break the cycle". - After aborting a transaction, the system must retry the aborted transaction after a random delay (100-500ms) to avoid repeated deadlocks.   4. Fairness and Starvation Prevention - Lock requests must be processed in FIFO order per resource: the first requester (lowest Lamport timestamp) gets the lock first. - Each resource must maintain a priority queue ordered by Lamport timestamp: {client_id, timestamp, mode, timeout_ms}. - Starvation prevention: If a client waits for more than 60 seconds (max_wait_time_ms), its priority must be boosted to the highest. - When a client's priority is boosted, it must be moved to the front of the queue and granted the lock as soon as the current holder releases it. - The system must track wait time per client per resource: {client_id, resource_id, wait_start_time, current_wait_time}. - When a lock is released, the next client in the FIFO queue must be notified within 10ms.
- Fault Tolerance and Recovery - The system must survive node failures: up to N/2 - 1 nodes can fail without losing lock state (e.g., 5 nodes can tolerate 2 failures). - Lock state must be replicated across a quorum of nodes: when a lock is granted, the state is written to quorum_size nodes. - Coordinator election: When the current coordinator fails, a new coordinator must be elected via Raft or Paxos within 5 seconds. - Recovery process: When a failed node rejoins, it must rebuild its lock state from the quorum within 10 seconds. - Recovery steps: (1) query quorum for current lock state, (2) sync local state, (3) resume processing requests. - When a network partition occurs, the majority partition must continue operating while the minority partition blocks all lock requests. - Split-brain prevention: Nodes in the minority partition must reject lock requests with error: "Network partition: not in majority partition".  . Lock Modes: SHARED and EXCLUSIVE - The system must support two lock modes: SHARED (read) and EXCLUSIVE (write). - Multiple SHARED locks can be held on the same resource simultaneously by different clients. - Only one EXCLUSIVE lock can be held on a resource at a time (no other SHARED or EXCLUSIVE locks allowed). - When a client holds a SHARED lock and requests an upgrade to EXCLUSIVE, it must wait for all other SHARED locks to be released. - Upgrade deadlock prevention: If two clients holding SHARED locks both request an upgrade to EXCLUSIVE, one must be aborted. - Downgrade operation: A client holding an EXCLUSIVE lock can downgrade to SHARED immediately without waiting. - Lock compatibility matrix:   - SHARED + SHARED = Compatible (both granted)   - SHARED + EXCLUSIVE = Incompatible (one waits)   - EXCLUSIVE + EXCLUSIVE = Incompatible (one waits)
- Lamport Timestamps for Total Ordering - Each lock request must be tagged with a Lamport timestamp: {logical_clock, node_id}. - Logical clock: Each node maintains a counter that increments on every event (send/receive message, local operation). - When a node receives a message with timestamp T, it must update its clock: local_clock = max(local_clock, T) + 1. - Total ordering: Requests are ordered first by logical_clock, then by node_id (lexicographic) to break ties. - When two requests have the same logical_clock, the request from the node with the smaller node_id is ordered first. - Lamport timestamps ensure FIFO ordering across distributed nodes without requiring synchronized clocks.  . Quorum Communication and Consensus - Lock acquisition requires quorum consensus: send request to all nodes, wait for quorum_size responses. - Quorum size: quorum_size = floor(N / 2) + 1 where N is the total number of nodes (e.g., 5 nodes â†’ quorum_size = 3). - When sending a lock request, the coordinator must send to all N nodes in parallel and wait for quorum_size responses. - Response types: GRANT (lock granted), DENY (lock held by another client), TIMEOUT (node unreachable). - If quorum_size nodes respond with GRANT within 50ms, the lock is acquired; otherwise, the request fails. - When a node is unreachable (timeout after 50ms), it is excluded from the quorum calculation for that request. - Quorum communication latency must be p99 < 30ms (time to receive quorum_size responses).
- 9. Heartbeat Mechanism for Lock Extension - Clients must send heartbeat(lock_token) every 10 seconds to extend the lock expiration. - Heartbeat message format: {lock_token, client_id, timestamp}. - When a heartbeat is received, the lock expiration must be extended by 30 seconds from the current time. - If no heartbeat is received for 15 seconds (1.5x heartbeat interval), the lock must be marked as expired. - Expired locks must be automatically released by a background cleanup thread that runs every 5 seconds. - When a lock is released due to timeout, the system must log: {timestamp, resource_id, client_id, reason: "heartbeat_timeout"}. - Heartbeat overhead must be < 1% CPU per node (measured via profiling).   Lock Token Format and Validation - Lock token format: {resource_id, client_id, mode, timestamp, version, expires_at, signature}. - Signature: HMAC-SHA256(resource_id + client_id + timestamp + version, secret_key) to prevent forgery. - When a client calls release_lock(resource_id, lock_token), the system must validate:   - (1) Signature is valid (HMAC matches)   - (2) Lock token matches the current lock holder   - (3) Lock has not expired (current_time < expires_at) - Invalid lock token error: "Invalid lock token: signature mismatch or lock expired". - Lock version: Incremented on each acquisition to detect stale tokens (e.g., client retries with old token).
- Configuration and Initialization - The system must load configuration from a JSON file with the following structure:   ```json   {     "cluster": {       "nodes": [         {"id": "node1", "host": "10.0.1.1", "port": 7001},         {"id": "node2", "host": "10.0.1.2", "port": 7001}       ],       "quorum_size": 3     },     "locks": {       "default_timeout_ms": 30000,       "heartbeat_interval_ms": 10000,       "deadlock_detection_interval_ms": 1000,       "max_wait_time_ms": 60000     }   }   ``` - On startup, each node must: (1) load config, (2) connect to other nodes, (3) elect coordinator, (4) start heartbeat thread, (5) start deadlock detection thread. - Configuration changes (adding/removing nodes) must be applied without restarting the cluster (hot reload).  ### 12. Error Handling and Logging - All errors must be logged with severity level (INFO, WARNING, ERROR, CRITICAL) and include: timestamp, node_id, operation, error_message. - Timeout during acquisition: Return None, log warning: "Lock acquisition timeout for resource_id={resource_id}, client_id={client_id}". - Quorum unavailable: Return None, log error: "Quorum unavailable: only 2/5 nodes reachable". - Invalid lock token: Return False, log error: "Invalid lock token: signature mismatch". - Deadlock detected: Log warning: "Deadlock detected: cycle={cycle}, aborting client={client_id}". - Node failure: Log error: "Node {node_id} failed, electing new coordinator". - Network partition: Log critical: "Network partition detected: operating with majority partition only".

## Metadata
- Programming Languages: Python
- Frameworks: (none)
- Libraries: (none)
- Databases: (none)
- Tools: (none)
- Best Practices: (none)
- Performance Metrics: (none)
- Security Standards: (none)

## Structure
- repository_before/: baseline code (`__init__.py`)
- repository_after/: optimized code (`__init__.py`)
- tests/: test suite (`__init__.py`)
- evaluation/: evaluation scripts (`evaluation.py`)
- instances/: sample/problem instances (JSON)
- patches/: patches for diffing
- trajectory/: notes or write-up (Markdown)

## Quick start
- Run tests locally: `python -m pytest -q tests`
- With Docker: `docker compose up --build --abort-on-container-exit`
- Add dependencies to `requirements.txt`

## Notes
- Keep commits focused and small.
- Open a PR when ready for review.
